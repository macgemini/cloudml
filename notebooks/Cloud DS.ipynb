{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Learning w chmurze - Cloud ML </h1>\n",
    "\n",
    "W tym notebooku pokażemy jak przenieść prosty model Tensorflow do GCP i uruchomić na nim predykcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predykcje na podstawie tekstu </h2>\n",
    "\n",
    "<b>Źródło danych</b>: Yelp Restaurant Reviews (https://www.yelp.com/dataset/challenge)\n",
    "\n",
    "Dataset zawiera między innymi informacje o restauracjach oraz opinie klientów\n",
    "\n",
    "Zadaniem jest przewidzenie czy restauracje przejdą inspekcje (amerykańskiego) sanepidu na podstawie opinii gości oraz dodatkowych informacji takich jak lokalizacja i rodzaje kuchni serwowanych w restauracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ustawienie zmiennych środowiskowych, import bibliotek </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BUCKET = 'dswbiznesie'\n",
    "PROJECT = 'dswbiznesie'\n",
    "REGION = 'europe-west1'\n",
    "REPO = '/content/datalab/dswbiznesie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['REPO'] = REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%datalab project set -p $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import google.datalab.ml as ml\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import datetime\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "import pandas as pd\n",
    "import google.datalab.bigquery as bq\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "#print tf.__version__\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dane źródłowe </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pobrany ze strony Yelp zawiera następujące pliki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 103941968  2017-11-06T00:52:39Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat\r\n",
      "    831242  2017-11-03T01:14:18Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat.additional\r\n",
      "    159046  2017-11-03T01:14:17Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat.labels\r\n",
      "TOTAL: 3 objects, 104932256 bytes (100.07 MiB)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/rawdata/hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>hygiene.dat</b>: każda linia zawiera połączone opinie klientów danej restauracji\n",
    "* <b>hygiene.dat.labels</b>: dla pierwszych 546 linii przypisana jest dodatkowe pole w którym 0 oznacza to że restauracja przeszła inspekcje, 1 to że restauracja <b>nie</b> przeszła inspekcji. Reszta wpisów posiada wpis \"[None]\" i nie będą brane pod uwagę przy obliczaniu modelu\n",
    "* <b>hygiene.dat.additional</b>: plik CSV gdzie w pierwszym polu znajduje się lista oferowanych rodzajów kuchnii, w drugim kod pocztowy restauracji który można uznać za przybliżenie lokalizacji restauracji. W trzecim polu znajduje się liczba opinii, w czwartym średnia ocena ( w skali 0-5, gdzie 5 oznacza ocene najlepszą)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature engineering używając Apache Beam i BigQuery</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane źródłowe należy przetwrzyć i dostosować do postaci której będzie można łatwo użyć do uczenia i ewaluacji modelu. \n",
    "Najwygodniejszym choć nie najtańszym rozwiązaniem jest załadowanie danych do BigQuery.\n",
    "\n",
    "Odpowiednim narzędziem do tego zadania jest Apache Beam i jego implementacja - Google Dataflow.\n",
    "Job Dataflow uruchamiany jest w chmurze. Jego przebieg można śledzić w Konsoli GCP (https://console.cloud.google.com/dataflow).\n",
    "Uruchomienie joba trwa powyżej minuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_record(rest_tuple):\n",
    "    #print(rest_tuple)\n",
    "    identity = rest_tuple[0]\n",
    "    reviews = rest_tuple[1]['reviews_kv'][0]\n",
    "    inspection_result = int(rest_tuple[1]['labels_kv'][0]) if rest_tuple[1]['labels_kv'][0] != \"[None]\" else None\n",
    "    categories_temp = rest_tuple[1]['attributes_kv'][0].split(\"\\\"\")\n",
    "    categories = \",\".join([ x.replace('\\'', '').replace('[','').replace(']','').strip() for x in categories_temp[1].split(',')])\n",
    "    #categories = categories_temp[1].replace('\\'', '').replace('[','').replace(']','')\n",
    "    attributes_temp = categories_temp[2].split(\",\")\n",
    "    zip_code = attributes_temp[1]\n",
    "    review_count = int(attributes_temp[2])\n",
    "    avg_rating = float(attributes_temp[3])\n",
    "    \n",
    "    return { \n",
    "        'identity': identity, \n",
    "        'reviews': reviews,\n",
    "        'inspection_result': inspection_result,\n",
    "        'categories': categories,\n",
    "        'zip_code': zip_code,\n",
    "        'review_count': review_count,\n",
    "        'avg_rating': avg_rating\n",
    "    }\n",
    "\n",
    "def preprocess(RUNNER,BUCKET,BIGQUERY_TABLE):\n",
    "    job_name = 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/data/hygiene/'.format(BUCKET)\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "        'project': PROJECT,\n",
    "        'region': 'europe-west1',\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    # Adding table definition\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields definition\n",
    "    identity_schema = bigquery.TableFieldSchema()\n",
    "    identity_schema.name = 'identity'\n",
    "    identity_schema.type = 'integer'\n",
    "    identity_schema.mode = 'required'\n",
    "    table_schema.fields.append(identity_schema)\n",
    "    \n",
    "    \n",
    "    reviews_schema = bigquery.TableFieldSchema()\n",
    "    reviews_schema.name = 'reviews'\n",
    "    reviews_schema.type = 'string'\n",
    "    reviews_schema.mode = 'required'\n",
    "    table_schema.fields.append(reviews_schema)\n",
    "\n",
    "    inspection_result_schema = bigquery.TableFieldSchema()\n",
    "    inspection_result_schema.name = 'inspection_result'\n",
    "    inspection_result_schema.type = 'integer'\n",
    "    inspection_result_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(inspection_result_schema)\n",
    "    \n",
    "    categories_schema = bigquery.TableFieldSchema()\n",
    "    categories_schema.name = 'categories'\n",
    "    categories_schema.type = 'string'\n",
    "    categories_schema.mode = 'required'\n",
    "    table_schema.fields.append(categories_schema)\n",
    "    \n",
    "    zip_code_schema = bigquery.TableFieldSchema()\n",
    "    zip_code_schema.name = 'zip_code'\n",
    "    zip_code_schema.type = 'string'\n",
    "    zip_code_schema.mode = 'required'\n",
    "    table_schema.fields.append(zip_code_schema)\n",
    "    \n",
    "    review_count_schema = bigquery.TableFieldSchema()\n",
    "    review_count_schema.name = 'review_count'\n",
    "    review_count_schema.type = 'integer'\n",
    "    review_count_schema.mode = 'required'\n",
    "    table_schema.fields.append(review_count_schema)\n",
    "    \n",
    "    avg_rating_schema = bigquery.TableFieldSchema()\n",
    "    avg_rating_schema.name = 'avg_rating'\n",
    "    avg_rating_schema.type = 'float'\n",
    "    avg_rating_schema.mode = 'required'\n",
    "    table_schema.fields.append(avg_rating_schema)\n",
    "    \n",
    "    #processing logic\n",
    "    reviews = p | 'Read Reviews' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat'.format(BUCKET))\n",
    "    labels = p | 'Read Labels' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.labels'.format(BUCKET))\n",
    "    attributes = p | 'Read Attributes' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.additional'.format(BUCKET))\n",
    "    \n",
    "    reviews_kv = reviews | 'Map Reviews to KV' >> beam.Map(lambda x: (x.split(\",\")[0], \",\".join(x.split(\",\")[1:]).replace(\"|\",\"\")))\n",
    "    labels_kv = labels | 'Map Labels to KV' >> beam.Map(lambda x: (x.split(\",\")[0], x.split(\",\")[1]))\n",
    "    attributes_kv = attributes | 'Map Attributes to KV' >> beam.Map(lambda x: (x.split(\",\")[0], x))\n",
    "    \n",
    "    restaurants = (\n",
    "        {'reviews_kv': reviews_kv, 'labels_kv': labels_kv, 'attributes_kv': attributes_kv}\n",
    "        | 'CoGroup By Restaurant Key' >> beam.CoGroupByKey())\n",
    "    \n",
    "    records = restaurants | 'Create Records' >> beam.Map(create_record)\n",
    "    \n",
    "    records | 'Write to BigQuery' >> beam.io.Write(\n",
    "        beam.io.BigQuerySink(\n",
    "            BIGQUERY_TABLE,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
    "    return p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Uruchomienie etl </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job hygiene-ftng-171118-193842 ... hang on\n"
     ]
    }
   ],
   "source": [
    "bigquery_dataset = \"{}:{}.hygiene\".format(PROJECT,PROJECT)\n",
    "#preprocess('DirectRunner',BUCKET, bigquery_dataset)\n",
    "job = preprocess('Dataflow', BUCKET, bigquery_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Przygotowanie danych do trenowania modelu </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pobranie danych do DataFrame (pandas) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  #identity,\n",
    "  reviews,\n",
    "  inspection_result ,\n",
    "  categories,\n",
    "  zip_code,\n",
    "  review_count,\n",
    "  avg_rating\n",
    "FROM\n",
    "  `dswbiznesie.hygiene`\n",
    "WHERE\n",
    "  inspection_result IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Podzial danych na zestaw treningowy i ewaluacyjny </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>inspection_result</th>\n",
       "      <th>categories</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went here on a first date which was a tad u...</td>\n",
       "      <td>0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>9</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not impressive. While the decor is pretty gar...</td>\n",
       "      <td>0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>3</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please note that this review only stems from ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Irish,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>37</td>\n",
       "      <td>3.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I grew up in the central valley of California...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm convinced this place needs to be on Guy F...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>15</td>\n",
       "      <td>3.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  inspection_result  \\\n",
       "0   I went here on a first date which was a tad u...                  0   \n",
       "1   Not impressive. While the decor is pretty gar...                  0   \n",
       "2   Please note that this review only stems from ...                  0   \n",
       "3   I grew up in the central valley of California...                  0   \n",
       "4   I'm convinced this place needs to be on Guy F...                  0   \n",
       "\n",
       "            categories zip_code  review_count  avg_rating  \n",
       "0          Restaurants    98101             9    3.111111  \n",
       "1          Restaurants    98101             3    2.666667  \n",
       "2    Irish,Restaurants    98101            37    3.302326  \n",
       "3  Mexican,Restaurants    98101             3    3.333333  \n",
       "4  Mexican,Restaurants    98101            15    3.200000  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) > 0\").execute().result().to_dataframe()\n",
    "evaldf  = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) = 0\").execute().result().to_dataframe()\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    205\n",
       "1    203\n",
       "Name: inspection_result, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    70\n",
       "0    68\n",
       "Name: inspection_result, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleanedtxt = text.replace(\"&#160;\",\" \").lower()\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  tokens = tokenizer.tokenize(cleanedtxt)\n",
    "  stop = stopwords.words('english')\n",
    "  nostoptokens = [i for i in tokens if i not in stop]\n",
    "  snowball = SnowballStemmer('english')\n",
    "  stems = [snowball.stem(i) for i in nostoptokens]\n",
    "  return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traindf['reviews'] = traindf['reviews'].apply(lambda x: preprocess_text(x))\n",
    "evaldf['reviews'] = evaldf['reviews'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='|')\n",
    "evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went first date tad uncomfort anyway went 8 00pm saturday music realli loud amp techno ish probabl ok like total alon haha ok enough mess coupl martini stoli dirti oliv ok like 3 oliv 2 c mon stingi kettl one w lemon said water realli know tri bad place definit date place chair w tall back hard atmospher nice modern servic great drink ok ladi got dress went w hotel bar last friday night place definit swanki prepar come dress impress cool side note fantasia barrino sit back tabl entourag crew tri hard stare much drink pretti spendi luckili happi hr start 10 pm mon fri enjoy tasti app definit tri sloppi joe like pull pork slider warn may intrigu sexi fri noth sexi sexi fri breath devour like limit garlic fri intak game safeco field needless say bit paranoid rest even get close person anyon confus place peopl seem flock yes chic cozi insid hotel bar popular seattl total conveni bf tire ventur downtown decid give w bar tri sinc could stumbl back upstair need signatur cocktail defin pricey 10 12 pop drink tasti other much look menu websit chang sinc drink similar secret martini citrus vodka lyche liquor lemon grapefruit juic probabl one best drink came high recommend bartend drink call bee sting come recommend bartend also cucumb midori cocktail fan chug 12 get like wasabi pea bar snack also decid tri w sexi fri 8 cri go agre yelper although yummi entir sure made sexi worth 8 want tri bar bite say wait happi hour seat comfort cozi sit lobbi hotel also get bar servic defin saw coupl snog couch though take upstair man tri eat sexi fri glass martini wine stay hotel big place nice decor w style truffl fri pretti good stay drink head dinner god bless w room servic 14 frakk dollar french fri 2x burger automat 22 gratuiti 4 servic fee sheet pillow great view meet interest peopl hallway trainwreck good stay cation ace hotel waaaay better cyclop blow door w bar shnitzel lunch drink dinner nice bar busi meet relax long airplan ride happi hour review hesit go w happi hour crowd night bit sceney mainten closur shucker put next door w hotel bar ladi hh surpris find quiet tuesday even seattl sog pour mayb quiet day friend order 2 specialti cocktail pear one 7 00 usual 12 juici yummi bite win brisket slider come 3 plate new addict falafel also 3 bit dri still tasti final chicken skewer 3 seem tasti could use littl dip sauc side close anoth specialti cocktail chai one bartend mix 1 2 n 1 2 make frothier honey sweet overal price seem pretti good hh complaint wine hh menu dessert alway item miss happi hour menus need sweet chocol item fun quiet particular night may work favor popular hh run tabl space make hard order eat nibbl largest seat arrang would fit mayb 7 8 bar servic seem prompt enough bartend came behind bar sever time check us alway appreci dark hip chic sexi seattl huh yes w hotel bar kind place loung money dispos glad come sip silli 12 cocktail manolo run tab worth car wish could rave w hotel experi best bf went v day one night got upstair room tri put key card sever time got red light key work frustrat long road trip want relax room top thing realli larg tall man open door ask tri get room stay mani mani hotel year never happen could front desk mess bf went downstair chang room appolig mix gave 2 free drink card bar area felt better point drink pricey kinda need drink get room horribl face brick wall money get crappi room crappi view realli sure ever stay locat like bar area though|0|Restaurants|98101|9|3.11111111111\r\n",
      "impress decor pretti garish light way bright scene midnight saturday night dead terribl w hotel need learn nation counterpart realli fix thing ambianc vibe everyth turn light make mood sexi modern play better music great spot good servic nice crowd destin definit place meet head expens happi hour short happi hour realli realli slow servic|0|Restaurants|98101|3|2.66666666667\r\n",
      "pleas note review stem visit weekend night drink food purchas happi hour pros great servic friend staff fun live band strong drinkscon need fan ventil hot warmer day time music touch loud get wrong fan loud music ear hurt shout night might touch loud go fun danc enjoy drink friend typic pub atmospher overal excel place kick back brew lot fun hope eventu tri happi hour well heard good thing love place big fan come downtown place love alley main street love guin tell someth love live band even best still great time get tabl challeng sometim hey busi got lucki sat night walk front tabl open race though kinda like steve martin kevin bacon go cab plane train amp automobil sweet servic alway good wee bit patient slam drink ask order number one piec advic waitress nice ask appl tart sorri two cider best well one right friend ask someth els menu end get one cider avail bake brie fruit plate fruit slice ridicul thin brie delici cider rememb name start also good might give one tri blown away first visit best part bar 2 side side walk narrow open room full tabl greet tune mother land albeit side get pretti hot tabl fill fast nice want relax drink good spirit listen uniqu live music side low key area tabl door tabl surround exterior place play top 40 hit style music lot cooler heat wise littl bit low key overal definit stop point whenev seattl portland met friend girlfriend meal night live music alway irish pub pour good pint kell keen food order pork tenderloin appl chutney fresh veget got mash potato gravi bad chop came butterfli make appear larger actual everyth good flavor serv worth 21 dislik seat area food one side bar seat larg group come drink make nice meal almost intoler want nice meal come earli bar realli start go music great side kell two bar tabl servic well check see play hand abl see great trio play sort irish tune one friendliest restaur visit seattl visit twice first time lunch husband saw sausag roll menu want tast home usual care pork bite realli good order veget pasti clam chowder solid clam chowder ruin rubberi clam much pepper one right soda bread realli nice waiter realli pleasant happi chat us minut make suggest thing see town guid book treat like annoy tourist like unwelcom riffraff big part end go back dinner last night town week heavi delici most food mix servic simpl bowl soup friend atmospher need place young un day would say poppin st patrick day know like regular basi definit place guess major holiday drink big part celebr surround great seattl attract various restaur drink kind expens would nice bar pub go whenev go kell mix emot love band play weekend like locat edg pike place market even mind crowd entertain watch drunken frat boy tri danc jig care overpr drink rude bouncer stop call friend walki talki point person hair person dress note bartend serv drink bouncer go snatch away minut later tri follow last call ok regular visit seattl rustic woodi irish bar although even strang reason charm quit nice day wait staff help kind servic quick sheppard pie tasti im alway fan corn beef cabbag defin great place visit wharf area pub crawl fare fine cuisin that look place want good healthi food cold beer kind folk great place start came quick drink birthday bar crawl difficult find locat dive bar type decor loud ambianc rude bartend bit pricey amaz appet fresh oyster chicken pasti potato farl wrap sever craft beer made nice littl even okay well may beauti outdoor seat number plastic chair tabl back alley half block pike place still nice respit throng peopl make way market given day great place stop guin three friend waitress nice vibe friend stop quick beer dinner pink door seen ad sounder fc program like support support boy impress ambianc price select definit pub feel lot dark wood space open encourag friend convers happi hour happen daili 4 7 draft hoegaarden sad 4 50 server great plan return night play live music dinner decor menu look authent ireland look forward tri potato farl went cowork sever time town work also alon find seat next door white hors tavern far away one favorit bar nation wide food ok standard pub grub atmospher bit lack better place around sort club feel friday saturday night live music start get pack most lack irish pub feel irish pub block away pike place market nice patio allow watch world go got small plate drink staff friend attent good choic irish brew well local craft beer tap delight place away sunni seattl afternoon stop quick drink dinner across alley pink door bar noth special live entertain could stay night singl middl age dude acoust guitar play great song full day walk everywher stop end night small bar anyth drink want dessert end get cheesecak made night feel awkward parti 8 asian peopl place seem like asian peopl servic realli good friday night also end get oyster melt mouth would go oyster expect come hang regular feel good type music differ explain banjo fiddl noth want warn ahead time oh yah suppos place haunt use mortuari becam bar waaaayyy back day unusu experi night awesom happi hour 3 item 3 50 well nice view super cute local came quick drink bartend know pour guin want done suprem technic friend talk place seattl want check like local put most im busi im friend busi well went month ago love felt right home staff nice food like good choic booz think made like histori place old build block away work would def come back anoth time rumor ghost never saw lol overal good time would love come back friend ive 1 time far cant tell best thing menu atleast take chanc dont like least like area great place alway good servic good food definit fave like beer ale place wine list okay noth write home food hearti hard superl menu tend toward american version irish standard find lamb shepherd pie instanc friday night bar get quit busi peopl watch decent stop weather turn bad decid drink would help us cope pretti dark insid televis plenti tabl chair bar pretti solid got beer bartend made drink price reason may order food given menus never came area look watch game get drink could place best chowder pike place market avoid crazi weekend crowd stop lunch weekday son love soda bread bonus mimosa offer monday stop place drink love atmospher went week pretti empti staff attent nice gave great suggest drink definit come back vacat stop cocktail bartend rori awesom made visit memor laugh much fun felt special well taken care kell decent bar fish chip came recommend drink enjoy depend time day night visit kell pack sometim difficult find place sit even stand realli enjoy kell recent busi trip seattl pasti sausag roll perfect snack guin ipa heard old funer parlor one time ad ambianc suspect return futur seattl trip especi sinc central locat near site conveni right near marriott water favorit place seattl bar none could manag would go coupl time week love food ambienc everyth place wait staff alway super friend help especi love live music everi night one thing could make place better would comfi leather armchair reserv friend bartend reason select nice custom went basebal game walk notic dark place even pub server nice handl lot tabl hard get attent need order check term liquor draft beer select limit order irish chees platter 4 chees good serv small corn beef pasti pretti goodnot fave irish pub probabl wont go back place good music realli sweet bartend terribl food hous wine dreck yes could presum enter still mussel realli bad even pub standard delici food good environ tabl servic kind slow well worth food love come sounder game guarante good time best sausag roll earthcorn beef cabbag alway good bet enjoy take break one afternoon though full ish waitress happili found us tabl drink enjoy move would certain go back waitress great god food disgust drink suck went 2 night row visit seattl great atmospher friend fun staff good food|0|Irish,Restaurants|98101|37|3.3023255814\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    138 eval.csv\r\n",
      "    408 train.csv\r\n",
      "    408 vocab.csv\r\n",
      "    954 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://eval.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/  1.2 MiB]                                                \r",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \r",
      "Copying file://train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.2 MiB/  4.4 MiB]                                                \r",
      "/ [2 files][  4.4 MiB/  4.4 MiB]                                                \r",
      "-\r",
      "Copying file://vocab.csv [Content-Type=text/csv]...\n",
      "- [2 files][  4.4 MiB/  7.6 MiB]                                                \r",
      "- [3 files][  7.6 MiB/  7.6 MiB]                                                \r\n",
      "Operation completed over 3 objects/7.6 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp *.csv gs://${BUCKET}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2017-11-03T00:08:16Z  gs://dswbiznesie/data/\r\n",
      "   1242719  2017-11-18T19:42:33Z  gs://dswbiznesie/data/eval.csv\r\n",
      "   3353088  2017-11-18T19:42:34Z  gs://dswbiznesie/data/train.csv\r\n",
      "   3353088  2017-11-18T19:42:34Z  gs://dswbiznesie/data/vocab.csv\r\n",
      "     39652  2017-11-11T18:14:43Z  gs://dswbiznesie/data/vocab_words\r\n",
      "                                 gs://dswbiznesie/data/hygiene/\r\n",
      "TOTAL: 5 objects, 7988547 bytes (7.62 MiB)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Tensorflow </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Właściwy model tensorflow znajduje się w pliku <b>model.py</b> a definicja joba Cloud ML w pliku <b>task.py</b>\n",
    "Kod poniżej ma za zadanie zilutrować działanie kodu tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "118 words into vocab.tsv\n",
      "This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels. --> [ 42  12  41 118  33 119  51  47 118 119  96  39 112  54   1  87 111  56\n",
      " 119  81  19 119  23  87 117  99  47 119]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "print tf.__version__\n",
    "MAX_DOCUMENT_LENGTH = 100000  \n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# vocabulary\n",
    "lines = ['This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels.', \n",
    "         'Im always looking for a good place to sing karaoke. This is one of them. Drinks are cheap. Food is delicious. And its laid back and fun. I shall return!', \n",
    "         'Crazy Man just Crazy there are like 3 different places called Saigon Deli at this intersection but this one is on Jackson just west of twelfthand with $2 pork Banh Mi you cant go wrong, I am going to Indiana for work so I went in and got 6 Pork', \n",
    "         'Ive eaten here a few times in the past and thought it was decent. I went last night, however, and our meal was really subpar so the place seems to have gone down hill.We had chow fun noodles and the hollow vegetables with chili sauce']\n",
    "\n",
    "lines = lines.map(lambda x: preprocess_text(x))\n",
    "\n",
    "# create vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(lines)\n",
    "with gfile.Open('vocab.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.iteritems():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "N_WORDS = len(vocab_processor.vocabulary_)\n",
    "print '{} words into vocab.tsv'.format(N_WORDS)\n",
    "\n",
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "numbers = table.lookup(tf.constant(lines[0].split()))\n",
    "with tf.Session() as sess:\n",
    "  tf.tables_initializer().run()\n",
    "  print \"{} --> {}\".format(lines[0], numbers.eval())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXW\r\n",
      "shop\r\n",
      "just\r\n",
      "cheap\r\n",
      "go\r\n",
      "Indiana\r\n",
      "its\r\n",
      "We\r\n",
      "seems\r\n",
      "laid\r\n"
     ]
    }
   ],
   "source": [
    "!head vocab.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie działania modelu na małym zbiorze danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=dswbiznesie\n",
      "5687 words into gs://dswbiznesie/data/vocab_words\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 19460), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "categories_out shape\n",
      "(?, 100, 1)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_avg_review:\n",
      "Tensor(\"l2_normalize:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_review_count:\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of categories:\n",
      "Tensor(\"Squeeze_3:0\", shape=(?, 100), dtype=float32)\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 19460), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "categories_out shape\n",
      "(?, 100, 1)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_avg_review:\n",
      "Tensor(\"l2_normalize:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_review_count:\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of categories:\n",
      "Tensor(\"Squeeze_3:0\", shape=(?, 100), dtype=float32)\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 19460), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "categories_out shape\n",
      "(?, 100, 1)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_avg_review:\n",
      "Tensor(\"l2_normalize:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_review_count:\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of categories:\n",
      "Tensor(\"Squeeze_3:0\", shape=(?, 100), dtype=float32)\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 19460), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "categories_out shape\n",
      "(?, 100, 1)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_avg_review:\n",
      "Tensor(\"l2_normalize:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_review_count:\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of categories:\n",
      "Tensor(\"Squeeze_3:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"string_to_index_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 19460), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "categories_out shape\n",
      "(?, 100, 1)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_2:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_avg_review:\n",
      "Tensor(\"l2_normalize:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of norm_review_count:\n",
      "Tensor(\"l2_normalize_1:0\", shape=(?, 1), dtype=float32)\n",
      "Shape of categories:\n",
      "Tensor(\"Squeeze_3:0\", shape=(?, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://dswbiznesie/data/train.csv...\n",
      "/ [0 files][    0.0 B/  3.2 MiB]                                                \r",
      "/ [1 files][  3.2 MiB/  3.2 MiB]                                                \r\n",
      "Operation completed over 1 objects/3.2 MiB.                                      \n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc359cd3f90>, '_model_dir': 'outputdir/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-11-19 01:06:31.584261: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-19 01:06:31.584304: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-19 01:06:31.584314: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into outputdir/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.713471, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-19-01:06:34\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-19-01:07:17\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.498044, f1score = 0.664922, global_step = 1, loss = 0.74523, precision = 0.498044, recall = 1.0\n",
      "INFO:tensorflow:Validation (step 1): loss = 0.74523, global_step = 1, recall = 1.0, precision = 0.498044, f1score = 0.664922, accuracy = 0.498044\n",
      "INFO:tensorflow:global_step/sec: 0.742791\n",
      "INFO:tensorflow:loss = 0.657556, step = 101 (134.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.10657\n",
      "INFO:tensorflow:loss = 0.599758, step = 201 (90.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.78565\n",
      "INFO:tensorflow:loss = 0.464115, step = 301 (127.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.10244\n",
      "INFO:tensorflow:loss = 0.468921, step = 401 (90.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.1007\n",
      "INFO:tensorflow:loss = 0.301708, step = 501 (90.851 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 574 into outputdir/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-19-01:16:34\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-574\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-19-01:17:17\n",
      "INFO:tensorflow:Saving dict for global step 574: accuracy = 0.616037, f1score = 0.604428, global_step = 574, loss = 0.853518, precision = 0.62069, recall = 0.589005\n",
      "INFO:tensorflow:Validation (step 574): loss = 0.853518, global_step = 574, recall = 0.589005, precision = 0.62069, f1score = 0.604428, accuracy = 0.616037\n",
      "INFO:tensorflow:global_step/sec: 0.742438\n",
      "INFO:tensorflow:loss = 0.135612, step = 601 (134.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.09344\n",
      "INFO:tensorflow:loss = 0.14541, step = 701 (91.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.09487\n",
      "INFO:tensorflow:loss = 0.0107369, step = 801 (91.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.875926\n",
      "INFO:tensorflow:loss = 2.94505e-05, step = 901 (114.165 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into outputdir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0468802.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-19-01:24:09\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-19-01:24:53\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.594524, f1score = 0.608307, global_step = 1000, loss = 1.33084, precision = 0.586165, recall = 0.632199\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: outputdir/export/Servo/1511054693/assets\n",
      "INFO:tensorflow:SavedModel written to: outputdir/export/Servo/1511054693/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf outputdir\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\n",
    "python -m trainer.task \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trenowanie modelu w Cloud ML </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać kod pythonowy działa przy bezpośrednim wywołaniu. Można przetestować go lokalnie w Cloud ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${REPO}/notebooks/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${REPO}/notebooks/trained_model \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Wizualizacja procesu uczenia w Tensorboard </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 5268. Click <a href=\"/_proxy/43452/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-11 18:00:48.963627. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('{}/notebooks/outputdir'.format(REPO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak przygotowany kod można uruchomić w klastrze Cloud ML. Status joba uczącego znajduje się w konsoli GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dswbiznesie/trained_model europe-west1 hygiene_171111_161537\n",
      "jobId: hygiene_171111_161537\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://dswbiznesie/trained_model/__init__.py#1510416808672460...\n",
      "Removing gs://dswbiznesie/trained_model/model.py#1510416808944571...\n",
      "Removing gs://dswbiznesie/trained_model/task.py#1510416809177912...\n",
      "/ [3/3 objects] 100% Done                                                       \n",
      "Operation completed over 3 objects.                                              \n",
      "Copying file://trainer/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file://trainer/model.py [Content-Type=text/x-python]...\n",
      "Copying file://trainer/task.py [Content-Type=text/x-python]...\n",
      "-\n",
      "Operation completed over 3 objects/12.8 KiB.                                     \n",
      "Job [hygiene_171111_161537] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hygiene_171111_161537\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hygiene_171111_161537\n",
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-11 16:15:37.157629. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/trained_model\n",
    "JOBNAME=hygiene_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gsutil cp trainer/*.py $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC --runtime-version=1.2 \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --train_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Opublikowanie wytrenowanego modelu </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/trained_model/export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"hygiene\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/trained_model/export/Servo/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Serwowanie predykcji </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opublikowany model, zostanie użyty do serwowania predykcji. Aby otrzymać predykcje, wystarczy wykonać JSONowy request do api predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./not-passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {\"instances\":\n",
    "  [\n",
    "      {\n",
    "      \"reviews\": preprocess_text(\" Great food, friendly staff, go go go go go! :) Ah, we always go here everytime we go to Pike Place. I always get either the Halibut sandwich or the Halibut platter.The sandwich comes in a french bread from Le Panier, delicious. The fish is fresh and why wouldn't it be? The Market Grill is inside pike place across from all the fish vendors! The cook takes it out and grills it in front of you with an option to add the blackening seasoning. I always get that. The platter comes with garlic bread, organic brown rice and organic salad. The rice is slightly flavored from the delicious dressing they have from the salad and it's soooo good. The fish is also very good.I've also had their clam chowder and I like it also.Ah, maybe I'll stop by this weekend. It's also a fun atmosphere because you can sit on the stools and watch people as you enjoy your fish. I've recommended this place to everyone I know! Especially if they're looking for something quick, cheaper and delicious in Pike Place. I've tried almost all of their sandwiches and I can honestly say they have been great. I've yet to try the chicken sandwich, but when your other options are salmon, halibut and prawns it's kind of hard to do so. It's totally worth grabbing the sandwich and walking around the market for a spot on a bench since the stools in their nook are almost always packed. I think there's only 8 of them. The salmon is probably my favorite but you really can't go wrong here. The chowder is also worth the wait. Make sure to treat yourself to the sides of chowder and coleslaw. You won't be disappointed. The staff here is usually very funny and talkative. They'll make you feel right at home, carrying on conversation while flipping your food on the grill just a few feet away from the counter. I bring my friends here whenever someone comes to visit and they all love it too. Just got back from a short mid-winter stay in Seattle. &#160;The weather was beautiful and we walked down to the Market for some lunch armed with several Yelp recommendations.We ended up at the Market Grill. &#160;Of course the place was crowded. &#160;We could have waited a few and grabbed one of the stools but opted to order take-out and eat in the park.In spite of some of the reviews indicating the sandwiches were a bit small, they looked big enough for us to share. &#160;We went with the salmon, blackened, and loaded with the onions, and rosmary mayo, lettuce, and tomatoes, two sides of slaw, and a medium chowder to share and headed to the park to eat our lunch. &#160;Total price $20.00I'm telling you, this is a damn good sandwich. &#160;Every component worked together from the crunchy baguette, the perfect tomatoes, the crisp lettuce, the MAYO (yum), and of course the huge piece of fresh perfectly cooked fish. &#160;The Slaw was crisp, chunky, and very flavorful, with the right amount of heat. &#160;The chowder was also very good. &#160;Definitely home made. &#160;Possibly a bit light on the clams, but then again, we finished the whole thing and were basically licking the bowl.We will come back - for sure.\")\n",
    "       },\n",
    "    {\n",
    "    \"reviews\":\" Pam's is really great. &#160;It's simple and delicious food that was perfect for a cold Seattle night that needed a little spicing up. &#160;I've only had the beef roti &#160;but I was so full that I couldn't fit any appetizer or dessert which I usually have room for. The beef roti was filled with curried chick peas, potatoes, large chunks of beef, and a yummy sauce. &#160;I can't wait to go back and try the other menu items. Also, A+ for service that was efficient, kind, and not overwhelming! Despite it's unassuming storefront facade, this is a very impressive little restaurant. &#160;Two of us tried two different dishes - lamb roti and a brown rice with roast peas - and both were perfect. For dessert we had an unusual and lovely little cake of coconut and casava root. The menu is simple, but every dish is excellent. &#160;It can't be easy to combine so many different influences (Carribean, African, East Indian) and still have the food taste so good and so subtle. Plus, on the rainy day that I was there, the mix of tropical colors, warm smells and Carribean music were a welcome change of atmosphere. I'm looking forward to going back. Really good food. When I ate meat, I tried both the chicken and beef. Both were really good. Now that I'm a vegetarian, I just eat the potato and chickpea mix and that is really good too. I've tried both styles of roti (wrapped up like a burrito and on the side like a big tortilla) and both are really good. The habanero hot sauce is amazing. I love it! Service is good too. I just wish it was closer to my apartment so I could eat there more often. The menu is limited, but what they do have is really good. They have perfected a small menu, which I think is much better than having a big mediocre menu. I find myself in Pam's Kitchen at least once per month, and am continuously amazed the place is not packed with people. &#160;The food is uniquely delicious and always exceptionally prepared - I usually order a Chicken Roti (Roti = chick peas potatoes and onions baked together with choice of meat and powerful blend of Caribbean spice, then wrapped in savory pastry) and one of their strangely addicting milk based punches, which come in peanut, carrot and (my favorite) pumpkin. &#160;At $15 for those two items (including tax and tip) it is a pretty good deal, though i realize you could get two mediocre teryaki meals from Tokyo Garden for the roughly the same price. &#160;They have a good selection of beer to fit the menu, service is great, and the island atmosphere is undeniably cool. &#160; Hungry? &#160;Thats good - I've never left Pam's feeling less than comfortably stuffed. &#160;The place is perfect for groups of three or four, so bring some friends and impress them with a creative and international dining experience on a student's budget. This place was fantastic! &#160;The owners were very nice and attentive--he even pointed out the Yelp! sticker on his window--what a business man!My friend and I decided this place looked interesting and we'd never had this kind of food before. &#160;Basically, we came looking for adventure and left satisfied. &#160;We split the Lamb Roti and were incredibly full. &#160;My friend thought it should be more saucy, but I was really happy w/it. &#160; &#160;Plus, the music is fantastic. &#160;At first I commented that it was way too loud, but then I kept dancing in my chair. &#160;It reminded me of some cheesy Bollywood-esque score. I'll admit that I'm naive about roti. &#160;So, I must make two disclaimers.First, I have no idea whether I'm being ripped off. &#160;I've read that, by island standards, this is way-expensive. &#160;Then again, this is Seattle, and living costs tend to make *everything* more expensive than on the streets of Chaguanas. &#160;It's not a steal (by even Seattle standards), but it also doesn't seem to be excessive.Second, I don't know how this holds up to roti in the Caribbean. &#160;It's hard to imagine that flaky goodness gets any flakier or goodness-er than this, but I hope it doesn't, because I fear for my carbohydrate intake once something better opens up.As you can probably already tell, I enjoyed Pam's Kitchen. &#160;I had the goat roti, with the version where some assembly is required (as opposed to the pre-filled roti, this one comes with all components separated). &#160;The goat was tender and reasonably juicy, and made for a perfect filling. &#160;I also got a spicy pumpkin side ($3), which was exactly as advertised, and a good complement to the main dish.Service for me was, contrary to some reviews, exemplary. &#160;They go by a sit-where-you-like system, but I was addressed promptly. &#160;I was also entertained by the cook briefly, who explained the positive medicinal effects of roti. &#160;I'm not sure if roti is responsible for curing all of the listed diseases, but if it is, I demand U.N. funding immediately, straight to my house.As mentioned, prices are moderate. &#160;Roti will run you $8 to $10, which seems high for the University District, but is about what this sort of food would run you elsewhere in town. &#160;Certainly, not enough to keep me away, but probably enough to deny me the prophylactic powers of Pam's delicacies.\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" OMG the spicy Teriyaki almost killed me today. &#160;I order it all the time and love it but this time it was like eating a fireball! &#160;I called to complain and they said \\\"next time we make not spicy\\\" &#160;What the heck? &#160;Also, someone please tell the cashier dude to clean his finger nails... Grosssssssss Hmmm should I go to Subway like always or try something new? Eh I might as well try something new. However, my first glance inside Teriyaki Plus isn't too encouraging as I don't see a soul inside. Eh, there are always rushes and slow periods. I go inside since there is no menu outside to look at. Immediately the cashier comes out to stare at me as if guilting me into buying something, instead of walking out after seeing the interior (dilapidated with dirty floors, and walls, along with tables and chairs that clearly need to be replaced). I order the white chicken and beef combo so that I can try each meat (in case one isn't very good). After waiting about 15 minutes (and getting annoyed because I got takeout since I'm in a hurry and yet have been waiting as long as I would have at a normal restaurant). The cashier comes out with my food and asks \\\"Fork?\\\" while motioning eating with a fork. I left with chopsticks (yes I'm a white girl who can use chopsticks) and my food in a bag. Upon opening the bag I noticed they used compostable containers which is nice, however the dressing from the salad had leaked into my bag. I opened the container and realized they have given me a TON of salad drenched in an overly sweet creamy sauce. Maybe they thought I needed more vegetables in my diet? While they also gave me a lot of rice, the meat was lacking compared to most teriyaki places. Which is fine as long as the meat is quality. However, the beef was barely tolerable and the chicken was even drier than the beef. Did they perhaps forget to add sauce? I never drench my food in anything, but this called for drastic measures. After drenching all the meat in soy sauce it was slight less dehydrated, but still as tough and chewy as before. My conclusion: how exactly is this place still open? Is it due to stupid people like me who don't check Yelp when they're starving and in a hurry? Well, back to good old Subway for me in times of desperate takeout (or Qdoba across the street).\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" Nothing is fresh about it! The quality of food is lower then the worst fast food place you've ever been to (and the smell is awful). Its dirty and If you are used to eating somewhat healthy you will feel sick. The reason this place has any customers at all is because for three 20+ stores office buildings there are only 4 places to eat. The way the food is made grosses me out; when I watch things being slapped on top of each other I wish I had no visibility into their kitchen. On top of that because people have limited options they charge a lot for what they offer.\"\n",
    "      },\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'hygiene', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
