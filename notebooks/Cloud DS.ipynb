{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Learning w chmurze - Cloud ML </h1>\n",
    "\n",
    "W tym notebooku pokażemy jak przenieść prosty model Tensorflow do GCP i uruchomić na nim predykcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predykcje na podstawie tekstu </h2>\n",
    "\n",
    "<b>Źródło danych</b>: Yelp Restaurant Reviews (https://www.yelp.com/dataset/challenge)\n",
    "\n",
    "Dataset zawiera między innymi informacje o restauracjach oraz opinie klientów\n",
    "\n",
    "Zadaniem jest przewidzenie czy restauracje przejdą inspekcje (amerykańskiego) sanepidu na podstawie opinii gości oraz dodatkowych informacji takich jak lokalizacja i rodzaje kuchni serwowanych w restauracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ustawienie zmiennych środowiskowych, import bibliotek </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BUCKET = 'dswbiznesie'\n",
    "PROJECT = 'dswbiznesie'\n",
    "REGION = 'europe-west1'\n",
    "REPO = '/content/datalab/dswbiznesie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['REPO'] = REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%datalab project set -p $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import google.datalab.ml as ml\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import datetime\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "import pandas as pd\n",
    "import google.datalab.bigquery as bq\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dane źródłowe </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pobrany ze strony Yelp zawiera następujące pliki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/rawdata/hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>hygiene.dat</b>: każda linia zawiera połączone opinie klientów danej restauracji\n",
    "* <b>hygiene.dat.labels</b>: dla pierwszych 546 linii przypisana jest dodatkowe pole w którym 0 oznacza to że restauracja przeszła inspekcje, 1 to że restauracja <b>nie</b> przeszła inspekcji. Reszta wpisów posiada wpis \"[None]\" co oznacza że należą do zbioru testowego\n",
    "* <b>hygiene.dat.additional</b>: plik CSV gdzie w pierwszym polu znajduje się lista oferowanych rodzajów kuchnii, w drugim kod pocztowy restauracji który można uznać za przybliżenie lokalizacji restauracji. W trzecim polu znajduje się liczba opinii, w czwartym średnia ocena ( w skali 0-5, gdzie 5 oznacza ocene najlepszą)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature engineering używając Apache Beam i BigQuery</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane źródłowe należy przetowrzyć i dostosować do postaci której będzie można łatwo użyć do uczenia i ewaluacji modelu. \n",
    "Najwygodniejszym choć nie najtańszym rozwiązaniem jest załadowanie danych do BigQuery.\n",
    "\n",
    "Odpowiednim narzędziem do tego zadania jest Apache Beam i jego implementacja - Google Dataflow.\n",
    "Job Dataflow uruchamiany jest w chmurze. Jego przebieg można śledzić w Konsoli GCP (https://console.cloud.google.com/dataflow).\n",
    "Uruchomienie joba trwa powyżej minuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_record(rest_tuple):\n",
    "    #print(rest_tuple)\n",
    "    identity = rest_tuple[0]\n",
    "    reviews = rest_tuple[1]['reviews_kv'][0]\n",
    "    inspection_result = int(rest_tuple[1]['labels_kv'][0]) if rest_tuple[1]['labels_kv'][0] != \"[None]\" else None\n",
    "    categories_temp = rest_tuple[1]['attributes_kv'][0].split(\"\\\"\")\n",
    "    categories = \",\".join([ x.replace('\\'', '').replace('[','').replace(']','').strip() for x in categories_temp[1].split(',')])\n",
    "    #categories = categories_temp[1].replace('\\'', '').replace('[','').replace(']','')\n",
    "    attributes_temp = categories_temp[2].split(\",\")\n",
    "    zip_code = attributes_temp[1]\n",
    "    review_count = int(attributes_temp[2])\n",
    "    avg_rating = float(attributes_temp[3])\n",
    "    \n",
    "    return { \n",
    "        'identity': identity, \n",
    "        'reviews': reviews,\n",
    "        'inspection_result': inspection_result,\n",
    "        'categories': categories,\n",
    "        'zip_code': zip_code,\n",
    "        'review_count': review_count,\n",
    "        'avg_rating': avg_rating\n",
    "    }\n",
    "\n",
    "def preprocess(RUNNER,BUCKET,BIGQUERY_TABLE):\n",
    "    job_name = 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/data/hygiene/'.format(BUCKET)\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "        'project': PROJECT,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    # Adding table definition\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields definition\n",
    "    identity_schema = bigquery.TableFieldSchema()\n",
    "    identity_schema.name = 'identity'\n",
    "    identity_schema.type = 'integer'\n",
    "    identity_schema.mode = 'required'\n",
    "    table_schema.fields.append(identity_schema)\n",
    "    \n",
    "    \n",
    "    reviews_schema = bigquery.TableFieldSchema()\n",
    "    reviews_schema.name = 'reviews'\n",
    "    reviews_schema.type = 'string'\n",
    "    reviews_schema.mode = 'required'\n",
    "    table_schema.fields.append(reviews_schema)\n",
    "\n",
    "    inspection_result_schema = bigquery.TableFieldSchema()\n",
    "    inspection_result_schema.name = 'inspection_result'\n",
    "    inspection_result_schema.type = 'integer'\n",
    "    inspection_result_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(inspection_result_schema)\n",
    "    \n",
    "    categories_schema = bigquery.TableFieldSchema()\n",
    "    categories_schema.name = 'categories'\n",
    "    categories_schema.type = 'string'\n",
    "    categories_schema.mode = 'required'\n",
    "    table_schema.fields.append(categories_schema)\n",
    "    \n",
    "    zip_code_schema = bigquery.TableFieldSchema()\n",
    "    zip_code_schema.name = 'zip_code'\n",
    "    zip_code_schema.type = 'string'\n",
    "    zip_code_schema.mode = 'required'\n",
    "    table_schema.fields.append(zip_code_schema)\n",
    "    \n",
    "    review_count_schema = bigquery.TableFieldSchema()\n",
    "    review_count_schema.name = 'review_count'\n",
    "    review_count_schema.type = 'integer'\n",
    "    review_count_schema.mode = 'required'\n",
    "    table_schema.fields.append(review_count_schema)\n",
    "    \n",
    "    avg_rating_schema = bigquery.TableFieldSchema()\n",
    "    avg_rating_schema.name = 'avg_rating'\n",
    "    avg_rating_schema.type = 'float'\n",
    "    avg_rating_schema.mode = 'required'\n",
    "    table_schema.fields.append(avg_rating_schema)\n",
    "    \n",
    "    #processing logic\n",
    "    reviews = p | 'readreviews' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat'.format(BUCKET))\n",
    "    labels = p | 'readlabels' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.labels'.format(BUCKET))\n",
    "    attributes = p | 'readattributes' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.additional'.format(BUCKET))\n",
    "    \n",
    "    reviews_kv = reviews | 'mapreviews to kv' >> beam.Map(lambda x: (x.split(\",\")[0], \",\".join(x.split(\",\")[1:]).replace(\"|\",\"\")))\n",
    "    labels_kv = labels | 'maplabels to kv' >> beam.Map(lambda x: (x.split(\",\")[0], x.split(\",\")[1]))\n",
    "    attributes_kv = attributes | 'mapattributes to kv' >> beam.Map(lambda x: (x.split(\",\")[0], x))\n",
    "    \n",
    "    restaurants = (\n",
    "        {'reviews_kv': reviews_kv, 'labels_kv': labels_kv, 'attributes_kv': attributes_kv}\n",
    "        | 'CoGroupByRestaurantKey' >> beam.CoGroupByKey())\n",
    "    \n",
    "    records = restaurants | 'CreateRecords' >> beam.Map(create_record)\n",
    "    \n",
    "    records | 'write' >> beam.io.Write(\n",
    "        beam.io.BigQuerySink(\n",
    "            BIGQUERY_TABLE,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
    "    p.run().wait_until_finish()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Uruchomienie etl </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigquery_dataset = PROJECT+\":\"+PROJECT+\".hygiene\"\n",
    "preprocess('DirectRunner',BUCKET, bigquery_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Przygotowanie danych do trenowania modelu </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pobranie danych do DataFrame (pandas) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  #identity,\n",
    "  reviews,\n",
    "  inspection_result #,\n",
    "  #categories,\n",
    "  #zip_code,\n",
    "  #review_count,\n",
    "  #avg_rating\n",
    "FROM\n",
    "  `dswbiznesie.hygiene`\n",
    "WHERE\n",
    "  inspection_result IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Podzial danych na zestaw treningowy i ewaluacyjny </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) > 0\").execute().result().to_dataframe()\n",
    "evaldf  = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) = 0\").execute().result().to_dataframe()\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaldf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='|')\n",
    "evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -3 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp *.csv gs://${BUCKET}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Tensorflow </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Właściwy model tensorflow znajduje się w pliku <b>model.py</b> a definicja joba Cloud ML w pliku <b>task.py</b>\n",
    "Kod poniżej ma za zadanie zilutrować działanie kodu tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "print tf.__version__\n",
    "MAX_DOCUMENT_LENGTH = 100000  \n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# vocabulary\n",
    "lines = ['This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels.', \n",
    "         'Im always looking for a good place to sing karaoke. This is one of them. Drinks are cheap. Food is delicious. And its laid back and fun. I shall return!', \n",
    "         'Crazy Man just Crazy there are like 3 different places called Saigon Deli at this intersection but this one is on Jackson just west of twelfthand with $2 pork Banh Mi you cant go wrong, I am going to Indiana for work so I went in and got 6 Pork', \n",
    "         'Ive eaten here a few times in the past and thought it was decent. I went last night, however, and our meal was really subpar so the place seems to have gone down hill.We had chow fun noodles and the hollow vegetables with chili sauce']\n",
    "\n",
    "# create vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(lines)\n",
    "with gfile.Open('vocab.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.iteritems():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "N_WORDS = len(vocab_processor.vocabulary_)\n",
    "print '{} words into vocab.tsv'.format(N_WORDS)\n",
    "\n",
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "numbers = table.lookup(tf.constant(lines[0].split()))\n",
    "with tf.Session() as sess:\n",
    "  tf.tables_initializer().run()\n",
    "  print \"{} --> {}\".format(lines[0], numbers.eval())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head vocab.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string operations\n",
    "reviews = tf.constant(lines)\n",
    "words = tf.string_split(reviews)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "# now pad out with zeros and then slice to constant length\n",
    "padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "padded = tf.pad(numbers, padding)\n",
    "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.tables_initializer().run()\n",
    "  print \"reviews=\", reviews.eval(), reviews.shape\n",
    "  print \"words=\", words.eval()\n",
    "  print \"dense=\", densewords.eval(), densewords.shape\n",
    "  print \"numbers=\", numbers.eval(), numbers.shape\n",
    "  print \"padding=\", padding.eval(), padding.shape\n",
    "  print \"padded=\", padded.eval(), padded.shape\n",
    "  print \"sliced=\", sliced.eval(), sliced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie działania modelu na małym zbiorze danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf outputdir\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\n",
    "python -m trainer.task \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trenowanie modelu w Cloud ML </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać kod pythonowy działa przy bezpośrednim wywołaniu. Można przetestować go lokalnie w Cloud ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${REPO}/notebooks/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${REPO}/notebooks/trained_model \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Wizualizacja procesu uczenia w Tensorboard </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('{}/notebooks/outputdir'.format(REPO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak przygotowany kod można uruchomić w klastrze Cloud ML. Status joba uczącego znajduje się w konsoli GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dswbiznesie/trained_model europe-west1 hygiene_171110_011442\n",
      "jobId: hygiene_171110_011442\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://dswbiznesie/trained_model/#1510275952669802...\n",
      "Removing gs://dswbiznesie/trained_model/__init__.py#1510275932342618...\n",
      "Removing gs://dswbiznesie/trained_model/checkpoint#1510275954709779...\n",
      "Removing gs://dswbiznesie/trained_model/events.out.tfevents.1510275947.cmle-training-master-6c7f81d3b4-0-fw6d1#1510276444188049...\n",
      "Removing gs://dswbiznesie/trained_model/graph.pbtxt#1510275949466569...\n",
      "Removing gs://dswbiznesie/trained_model/model.ckpt-1.data-00000-of-00001#1510275953108291...\n",
      "Removing gs://dswbiznesie/trained_model/model.ckpt-1.index#1510275953515223...\n",
      "Removing gs://dswbiznesie/trained_model/model.ckpt-1.meta#1510275955789208...\n",
      "Removing gs://dswbiznesie/trained_model/model.py#1510275932590897...\n",
      "Removing gs://dswbiznesie/trained_model/task.py#1510275932844604...\n",
      "/ [1/10 objects]  10% Done                                                      \r",
      "/ [2/10 objects]  20% Done                                                      \r",
      "/ [3/10 objects]  30% Done                                                      \r",
      "/ [4/10 objects]  40% Done                                                      \r",
      "/ [5/10 objects]  50% Done                                                      \r",
      "/ [6/10 objects]  60% Done                                                      \r",
      "/ [7/10 objects]  70% Done                                                      \r",
      "/ [8/10 objects]  80% Done                                                      \r",
      "/ [9/10 objects]  90% Done                                                      \r",
      "/ [10/10 objects] 100% Done                                                     \r\n",
      "Operation completed over 10 objects.                                             \n",
      "Copying file://trainer/__init__.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  677.0 B]                                                \r",
      "/ [1 files][  677.0 B/  677.0 B]                                                \r",
      "Copying file://trainer/model.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  677.0 B/  7.4 KiB]                                                \r",
      "/ [2 files][  7.4 KiB/  7.4 KiB]                                                \r",
      "Copying file://trainer/task.py [Content-Type=text/x-python]...\n",
      "/ [2 files][  7.4 KiB/  8.9 KiB]                                                \r",
      "/ [3 files][  8.9 KiB/  8.9 KiB]                                                \r",
      "-\r\n",
      "Operation completed over 3 objects/8.9 KiB.                                      \n",
      "Job [hygiene_171110_011442] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hygiene_171110_011442\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hygiene_171110_011442\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/trained_model\n",
    "JOBNAME=hygiene_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gsutil cp trainer/*.py $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC --runtime-version=1.2 \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --train_steps=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Opublikowanie wytrenowanego modelu </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dswbiznesie/trained_model/export/Servo/\n",
      "gs://dswbiznesie/trained_model/export/Servo/1510276532/\n",
      "gs://dswbiznesie/trained_model/export/Servo/1510276590/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/trained_model/export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying hygiene v1 from gs://dswbiznesie/trained_model/export/Servo/1510276590/ ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/dswbiznesie/models/hygiene].\n",
      "Creating version (this might take a few minutes)......\n",
      "..........................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"hygiene\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/trained_model/export/Servo/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Serwowanie predykcji </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opublikowany model, zostanie użyty do serwowania predykcji. Aby otrzymać predykcje, wystarczy wykonać JSONowy request do api predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS  PROB                                       SOURCE\n",
      "1      [0.12512125074863434, 0.8748787641525269]  0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS  PROB                                       SOURCE\n",
      "1      [0.08528221398591995, 0.9147177934646606]  0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./not-passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 404 when requesting https://ml.googleapis.com/v1beta1/projects/dswbiznesie/models/hygiene/versions/v1:predict?alt=json returned \"Not Found\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0626d68a4394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'projects/%s/models/%s/versions/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPROJECT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hygiene'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"response={0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/oauth2client/util.pyc\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/googleapiclient/http.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 404 when requesting https://ml.googleapis.com/v1beta1/projects/dswbiznesie/models/hygiene/versions/v1:predict?alt=json returned \"Not Found\">"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {\"instances\":\n",
    "  [\n",
    "      {\n",
    "      \"reviews\":\" Great food, friendly staff, go go go go go! :) Ah, we always go here everytime we go to Pike Place. I always get either the Halibut sandwich or the Halibut platter.The sandwich comes in a french bread from Le Panier, delicious. The fish is fresh and why wouldn't it be? The Market Grill is inside pike place across from all the fish vendors! The cook takes it out and grills it in front of you with an option to add the blackening seasoning. I always get that. The platter comes with garlic bread, organic brown rice and organic salad. The rice is slightly flavored from the delicious dressing they have from the salad and it's soooo good. The fish is also very good.I've also had their clam chowder and I like it also.Ah, maybe I'll stop by this weekend. It's also a fun atmosphere because you can sit on the stools and watch people as you enjoy your fish. I've recommended this place to everyone I know! Especially if they're looking for something quick, cheaper and delicious in Pike Place. I've tried almost all of their sandwiches and I can honestly say they have been great. I've yet to try the chicken sandwich, but when your other options are salmon, halibut and prawns it's kind of hard to do so. It's totally worth grabbing the sandwich and walking around the market for a spot on a bench since the stools in their nook are almost always packed. I think there's only 8 of them. The salmon is probably my favorite but you really can't go wrong here. The chowder is also worth the wait. Make sure to treat yourself to the sides of chowder and coleslaw. You won't be disappointed. The staff here is usually very funny and talkative. They'll make you feel right at home, carrying on conversation while flipping your food on the grill just a few feet away from the counter. I bring my friends here whenever someone comes to visit and they all love it too. Just got back from a short mid-winter stay in Seattle. &#160;The weather was beautiful and we walked down to the Market for some lunch armed with several Yelp recommendations.We ended up at the Market Grill. &#160;Of course the place was crowded. &#160;We could have waited a few and grabbed one of the stools but opted to order take-out and eat in the park.In spite of some of the reviews indicating the sandwiches were a bit small, they looked big enough for us to share. &#160;We went with the salmon, blackened, and loaded with the onions, and rosmary mayo, lettuce, and tomatoes, two sides of slaw, and a medium chowder to share and headed to the park to eat our lunch. &#160;Total price $20.00I'm telling you, this is a damn good sandwich. &#160;Every component worked together from the crunchy baguette, the perfect tomatoes, the crisp lettuce, the MAYO (yum), and of course the huge piece of fresh perfectly cooked fish. &#160;The Slaw was crisp, chunky, and very flavorful, with the right amount of heat. &#160;The chowder was also very good. &#160;Definitely home made. &#160;Possibly a bit light on the clams, but then again, we finished the whole thing and were basically licking the bowl.We will come back - for sure.\"\n",
    "       },\n",
    "    {\n",
    "    \"reviews\":\" Pam's is really great. &#160;It's simple and delicious food that was perfect for a cold Seattle night that needed a little spicing up. &#160;I've only had the beef roti &#160;but I was so full that I couldn't fit any appetizer or dessert which I usually have room for. The beef roti was filled with curried chick peas, potatoes, large chunks of beef, and a yummy sauce. &#160;I can't wait to go back and try the other menu items. Also, A+ for service that was efficient, kind, and not overwhelming! Despite it's unassuming storefront facade, this is a very impressive little restaurant. &#160;Two of us tried two different dishes - lamb roti and a brown rice with roast peas - and both were perfect. For dessert we had an unusual and lovely little cake of coconut and casava root. The menu is simple, but every dish is excellent. &#160;It can't be easy to combine so many different influences (Carribean, African, East Indian) and still have the food taste so good and so subtle. Plus, on the rainy day that I was there, the mix of tropical colors, warm smells and Carribean music were a welcome change of atmosphere. I'm looking forward to going back. Really good food. When I ate meat, I tried both the chicken and beef. Both were really good. Now that I'm a vegetarian, I just eat the potato and chickpea mix and that is really good too. I've tried both styles of roti (wrapped up like a burrito and on the side like a big tortilla) and both are really good. The habanero hot sauce is amazing. I love it! Service is good too. I just wish it was closer to my apartment so I could eat there more often. The menu is limited, but what they do have is really good. They have perfected a small menu, which I think is much better than having a big mediocre menu. I find myself in Pam's Kitchen at least once per month, and am continuously amazed the place is not packed with people. &#160;The food is uniquely delicious and always exceptionally prepared - I usually order a Chicken Roti (Roti = chick peas potatoes and onions baked together with choice of meat and powerful blend of Caribbean spice, then wrapped in savory pastry) and one of their strangely addicting milk based punches, which come in peanut, carrot and (my favorite) pumpkin. &#160;At $15 for those two items (including tax and tip) it is a pretty good deal, though i realize you could get two mediocre teryaki meals from Tokyo Garden for the roughly the same price. &#160;They have a good selection of beer to fit the menu, service is great, and the island atmosphere is undeniably cool. &#160; Hungry? &#160;Thats good - I've never left Pam's feeling less than comfortably stuffed. &#160;The place is perfect for groups of three or four, so bring some friends and impress them with a creative and international dining experience on a student's budget. This place was fantastic! &#160;The owners were very nice and attentive--he even pointed out the Yelp! sticker on his window--what a business man!My friend and I decided this place looked interesting and we'd never had this kind of food before. &#160;Basically, we came looking for adventure and left satisfied. &#160;We split the Lamb Roti and were incredibly full. &#160;My friend thought it should be more saucy, but I was really happy w/it. &#160; &#160;Plus, the music is fantastic. &#160;At first I commented that it was way too loud, but then I kept dancing in my chair. &#160;It reminded me of some cheesy Bollywood-esque score. I'll admit that I'm naive about roti. &#160;So, I must make two disclaimers.First, I have no idea whether I'm being ripped off. &#160;I've read that, by island standards, this is way-expensive. &#160;Then again, this is Seattle, and living costs tend to make *everything* more expensive than on the streets of Chaguanas. &#160;It's not a steal (by even Seattle standards), but it also doesn't seem to be excessive.Second, I don't know how this holds up to roti in the Caribbean. &#160;It's hard to imagine that flaky goodness gets any flakier or goodness-er than this, but I hope it doesn't, because I fear for my carbohydrate intake once something better opens up.As you can probably already tell, I enjoyed Pam's Kitchen. &#160;I had the goat roti, with the version where some assembly is required (as opposed to the pre-filled roti, this one comes with all components separated). &#160;The goat was tender and reasonably juicy, and made for a perfect filling. &#160;I also got a spicy pumpkin side ($3), which was exactly as advertised, and a good complement to the main dish.Service for me was, contrary to some reviews, exemplary. &#160;They go by a sit-where-you-like system, but I was addressed promptly. &#160;I was also entertained by the cook briefly, who explained the positive medicinal effects of roti. &#160;I'm not sure if roti is responsible for curing all of the listed diseases, but if it is, I demand U.N. funding immediately, straight to my house.As mentioned, prices are moderate. &#160;Roti will run you $8 to $10, which seems high for the University District, but is about what this sort of food would run you elsewhere in town. &#160;Certainly, not enough to keep me away, but probably enough to deny me the prophylactic powers of Pam's delicacies.\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" OMG the spicy Teriyaki almost killed me today. &#160;I order it all the time and love it but this time it was like eating a fireball! &#160;I called to complain and they said \\\"next time we make not spicy\\\" &#160;What the heck? &#160;Also, someone please tell the cashier dude to clean his finger nails... Grosssssssss Hmmm should I go to Subway like always or try something new? Eh I might as well try something new. However, my first glance inside Teriyaki Plus isn't too encouraging as I don't see a soul inside. Eh, there are always rushes and slow periods. I go inside since there is no menu outside to look at. Immediately the cashier comes out to stare at me as if guilting me into buying something, instead of walking out after seeing the interior (dilapidated with dirty floors, and walls, along with tables and chairs that clearly need to be replaced). I order the white chicken and beef combo so that I can try each meat (in case one isn't very good). After waiting about 15 minutes (and getting annoyed because I got takeout since I'm in a hurry and yet have been waiting as long as I would have at a normal restaurant). The cashier comes out with my food and asks \\\"Fork?\\\" while motioning eating with a fork. I left with chopsticks (yes I'm a white girl who can use chopsticks) and my food in a bag. Upon opening the bag I noticed they used compostable containers which is nice, however the dressing from the salad had leaked into my bag. I opened the container and realized they have given me a TON of salad drenched in an overly sweet creamy sauce. Maybe they thought I needed more vegetables in my diet? While they also gave me a lot of rice, the meat was lacking compared to most teriyaki places. Which is fine as long as the meat is quality. However, the beef was barely tolerable and the chicken was even drier than the beef. Did they perhaps forget to add sauce? I never drench my food in anything, but this called for drastic measures. After drenching all the meat in soy sauce it was slight less dehydrated, but still as tough and chewy as before. My conclusion: how exactly is this place still open? Is it due to stupid people like me who don't check Yelp when they're starving and in a hurry? Well, back to good old Subway for me in times of desperate takeout (or Qdoba across the street).\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" Nothing is fresh about it! The quality of food is lower then the worst fast food place you've ever been to (and the smell is awful). Its dirty and If you are used to eating somewhat healthy you will feel sick. The reason this place has any customers at all is because for three 20+ stores office buildings there are only 4 places to eat. The way the food is made grosses me out; when I watch things being slapped on top of each other I wish I had no visibility into their kitchen. On top of that because people have limited options they charge a lot for what they offer.\"\n",
    "      },\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'hygiene', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
