{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Machine Learning w chmurze - Cloud ML </h1>\n",
    "\n",
    "W tym notebooku pokażemy jak przenieść prosty model Tensorflow do GCP i uruchomić na nim predykcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predykcje na podstawie tekstu </h2>\n",
    "\n",
    "<b>Źródło danych</b>: Yelp Restaurant Reviews (https://www.yelp.com/dataset/challenge)\n",
    "\n",
    "Dataset zawiera między innymi informacje o restauracjach oraz opinie klientów\n",
    "\n",
    "Zadaniem jest przewidzenie czy restauracje przejdą inspekcje (amerykańskiego) sanepidu na podstawie opinii gości oraz dodatkowych informacji takich jak lokalizacja i rodzaje kuchni serwowanych w restauracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ustawienie zmiennych środowiskowych, import bibliotek </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BUCKET = 'dswbiznesie'\n",
    "PROJECT = 'dswbiznesie'\n",
    "REGION = 'europe-west1'\n",
    "REPO = '/content/datalab/dswbiznesie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['REPO'] = REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%datalab project set -p $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import google.datalab.ml as ml\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import datetime\n",
    "from apache_beam.io.gcp.internal.clients import bigquery\n",
    "import pandas as pd\n",
    "import google.datalab.bigquery as bq\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "#print tf.__version__\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dane źródłowe </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pobrany ze strony Yelp zawiera następujące pliki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 103941968  2017-11-06T00:52:39Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat\r\n",
      "    831242  2017-11-03T01:14:18Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat.additional\r\n",
      "    159046  2017-11-03T01:14:17Z  gs://dswbiznesie/rawdata/hygiene/hygiene.dat.labels\r\n",
      "TOTAL: 3 objects, 104932256 bytes (100.07 MiB)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/rawdata/hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>hygiene.dat</b>: każda linia zawiera połączone opinie klientów danej restauracji\n",
    "* <b>hygiene.dat.labels</b>: dla pierwszych 546 linii przypisana jest dodatkowe pole w którym 0 oznacza to że restauracja przeszła inspekcje, 1 to że restauracja <b>nie</b> przeszła inspekcji. Reszta wpisów posiada wpis \"[None]\" i nie będą brane pod uwagę przy obliczaniu modelu\n",
    "* <b>hygiene.dat.additional</b>: plik CSV gdzie w pierwszym polu znajduje się lista oferowanych rodzajów kuchnii, w drugim kod pocztowy restauracji który można uznać za przybliżenie lokalizacji restauracji. W trzecim polu znajduje się liczba opinii, w czwartym średnia ocena ( w skali 0-5, gdzie 5 oznacza ocene najlepszą)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature engineering używając Apache Beam i BigQuery</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane źródłowe należy przetwrzyć i dostosować do postaci której będzie można łatwo użyć do uczenia i ewaluacji modelu. \n",
    "Najwygodniejszym choć nie najtańszym rozwiązaniem jest załadowanie danych do BigQuery.\n",
    "\n",
    "Odpowiednim narzędziem do tego zadania jest Apache Beam i jego implementacja - Google Dataflow.\n",
    "Job Dataflow uruchamiany jest w chmurze. Jego przebieg można śledzić w Konsoli GCP (https://console.cloud.google.com/dataflow).\n",
    "Uruchomienie joba trwa powyżej minuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_record(rest_tuple):\n",
    "    #print(rest_tuple)\n",
    "    identity = rest_tuple[0]\n",
    "    reviews = rest_tuple[1]['reviews_kv'][0]\n",
    "    inspection_result = int(rest_tuple[1]['labels_kv'][0]) if rest_tuple[1]['labels_kv'][0] != \"[None]\" else None\n",
    "    categories_temp = rest_tuple[1]['attributes_kv'][0].split(\"\\\"\")\n",
    "    categories = \",\".join([ x.replace('\\'', '').replace('[','').replace(']','').strip() for x in categories_temp[1].split(',')])\n",
    "    #categories = categories_temp[1].replace('\\'', '').replace('[','').replace(']','')\n",
    "    attributes_temp = categories_temp[2].split(\",\")\n",
    "    zip_code = attributes_temp[1]\n",
    "    review_count = int(attributes_temp[2])\n",
    "    avg_rating = float(attributes_temp[3])\n",
    "    \n",
    "    return { \n",
    "        'identity': identity, \n",
    "        'reviews': reviews,\n",
    "        'inspection_result': inspection_result,\n",
    "        'categories': categories,\n",
    "        'zip_code': zip_code,\n",
    "        'review_count': review_count,\n",
    "        'avg_rating': avg_rating\n",
    "    }\n",
    "\n",
    "def preprocess(RUNNER,BUCKET,BIGQUERY_TABLE):\n",
    "    job_name = 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/data/hygiene/'.format(BUCKET)\n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': 'hygiene-ftng' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "        'project': PROJECT,\n",
    "        'region': 'europe-west1',\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    p = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    # Adding table definition\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields definition\n",
    "    identity_schema = bigquery.TableFieldSchema()\n",
    "    identity_schema.name = 'identity'\n",
    "    identity_schema.type = 'integer'\n",
    "    identity_schema.mode = 'required'\n",
    "    table_schema.fields.append(identity_schema)\n",
    "    \n",
    "    \n",
    "    reviews_schema = bigquery.TableFieldSchema()\n",
    "    reviews_schema.name = 'reviews'\n",
    "    reviews_schema.type = 'string'\n",
    "    reviews_schema.mode = 'required'\n",
    "    table_schema.fields.append(reviews_schema)\n",
    "\n",
    "    inspection_result_schema = bigquery.TableFieldSchema()\n",
    "    inspection_result_schema.name = 'inspection_result'\n",
    "    inspection_result_schema.type = 'integer'\n",
    "    inspection_result_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(inspection_result_schema)\n",
    "    \n",
    "    categories_schema = bigquery.TableFieldSchema()\n",
    "    categories_schema.name = 'categories'\n",
    "    categories_schema.type = 'string'\n",
    "    categories_schema.mode = 'required'\n",
    "    table_schema.fields.append(categories_schema)\n",
    "    \n",
    "    zip_code_schema = bigquery.TableFieldSchema()\n",
    "    zip_code_schema.name = 'zip_code'\n",
    "    zip_code_schema.type = 'string'\n",
    "    zip_code_schema.mode = 'required'\n",
    "    table_schema.fields.append(zip_code_schema)\n",
    "    \n",
    "    review_count_schema = bigquery.TableFieldSchema()\n",
    "    review_count_schema.name = 'review_count'\n",
    "    review_count_schema.type = 'integer'\n",
    "    review_count_schema.mode = 'required'\n",
    "    table_schema.fields.append(review_count_schema)\n",
    "    \n",
    "    avg_rating_schema = bigquery.TableFieldSchema()\n",
    "    avg_rating_schema.name = 'avg_rating'\n",
    "    avg_rating_schema.type = 'float'\n",
    "    avg_rating_schema.mode = 'required'\n",
    "    table_schema.fields.append(avg_rating_schema)\n",
    "    \n",
    "    #processing logic\n",
    "    reviews = p | 'Read Reviews' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat'.format(BUCKET))\n",
    "    labels = p | 'Read Labels' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.labels'.format(BUCKET))\n",
    "    attributes = p | 'Read Attributes' >> beam.io.ReadFromText('gs://{0}/rawdata/hygiene/hygiene.dat.additional'.format(BUCKET))\n",
    "    \n",
    "    reviews_kv = reviews | 'Map Reviews to KV' >> beam.Map(lambda x: (x.split(\",\")[0], \",\".join(x.split(\",\")[1:]).replace(\"|\",\"\")))\n",
    "    labels_kv = labels | 'Map Labels to KV' >> beam.Map(lambda x: (x.split(\",\")[0], x.split(\",\")[1]))\n",
    "    attributes_kv = attributes | 'Map Attributes to KV' >> beam.Map(lambda x: (x.split(\",\")[0], x))\n",
    "    \n",
    "    restaurants = (\n",
    "        {'reviews_kv': reviews_kv, 'labels_kv': labels_kv, 'attributes_kv': attributes_kv}\n",
    "        | 'CoGroup By Restaurant Key' >> beam.CoGroupByKey())\n",
    "    \n",
    "    records = restaurants | 'Create Records' >> beam.Map(create_record)\n",
    "    \n",
    "    records | 'Write to BigQuery' >> beam.io.Write(\n",
    "        beam.io.BigQuerySink(\n",
    "            BIGQUERY_TABLE,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
    "    return p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Uruchomienie etl </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job hygiene-ftng-171118-193842 ... hang on\n"
     ]
    }
   ],
   "source": [
    "bigquery_dataset = \"{}:{}.hygiene\".format(PROJECT,PROJECT)\n",
    "#preprocess('DirectRunner',BUCKET, bigquery_dataset)\n",
    "job = preprocess('Dataflow', BUCKET, bigquery_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Przygotowanie danych do trenowania modelu </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pobranie danych do DataFrame (pandas) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  #identity,\n",
    "  reviews,\n",
    "  inspection_result ,\n",
    "  categories,\n",
    "  zip_code,\n",
    "  review_count,\n",
    "  avg_rating\n",
    "FROM\n",
    "  `dswbiznesie.hygiene`\n",
    "WHERE\n",
    "  inspection_result IS NOT NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Podzial danych na zestaw treningowy i ewaluacyjny </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>inspection_result</th>\n",
       "      <th>categories</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not impressive. While the decor is pretty gar...</td>\n",
       "      <td>0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>3</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I went here on a first date which was a tad u...</td>\n",
       "      <td>0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>9</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please note that this review only stems from ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Irish,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>37</td>\n",
       "      <td>3.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm convinced this place needs to be on Guy F...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>15</td>\n",
       "      <td>3.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I grew up in the central valley of California...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexican,Restaurants</td>\n",
       "      <td>98101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  inspection_result  \\\n",
       "0   Not impressive. While the decor is pretty gar...                  0   \n",
       "1   I went here on a first date which was a tad u...                  0   \n",
       "2   Please note that this review only stems from ...                  0   \n",
       "3   I'm convinced this place needs to be on Guy F...                  0   \n",
       "4   I grew up in the central valley of California...                  0   \n",
       "\n",
       "            categories zip_code  review_count  avg_rating  \n",
       "0          Restaurants    98101             3    2.666667  \n",
       "1          Restaurants    98101             9    3.111111  \n",
       "2    Irish,Restaurants    98101            37    3.302326  \n",
       "3  Mexican,Restaurants    98101            15    3.200000  \n",
       "4  Mexican,Restaurants    98101             3    3.333333  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) > 0\").execute().result().to_dataframe()\n",
    "evaldf  = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(reviews)),4) = 0\").execute().result().to_dataframe()\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    205\n",
       "1    203\n",
       "Name: inspection_result, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    70\n",
       "0    68\n",
       "Name: inspection_result, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['inspection_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='|')\n",
    "evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -3 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    138 eval.csv\r\n",
      "    408 train.csv\r\n",
      "    408 vocab.csv\r\n",
      "    954 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://eval.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/  1.2 MiB]                                                \r",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \r",
      "Copying file://train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.2 MiB/  4.4 MiB]                                                \r",
      "/ [2 files][  4.4 MiB/  4.4 MiB]                                                \r",
      "-\r",
      "Copying file://vocab.csv [Content-Type=text/csv]...\n",
      "- [2 files][  4.4 MiB/  7.6 MiB]                                                \r",
      "- [3 files][  7.6 MiB/  7.6 MiB]                                                \r\n",
      "Operation completed over 3 objects/7.6 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp *.csv gs://${BUCKET}/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2017-11-03T00:08:16Z  gs://dswbiznesie/data/\r\n",
      "   1242719  2017-11-18T19:42:33Z  gs://dswbiznesie/data/eval.csv\r\n",
      "   3353088  2017-11-18T19:42:34Z  gs://dswbiznesie/data/train.csv\r\n",
      "   3353088  2017-11-18T19:42:34Z  gs://dswbiznesie/data/vocab.csv\r\n",
      "     39652  2017-11-11T18:14:43Z  gs://dswbiznesie/data/vocab_words\r\n",
      "                                 gs://dswbiznesie/data/hygiene/\r\n",
      "TOTAL: 5 objects, 7988547 bytes (7.62 MiB)\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil -q ls -l gs://$BUCKET/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Tensorflow </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Właściwy model tensorflow znajduje się w pliku <b>model.py</b> a definicja joba Cloud ML w pliku <b>task.py</b>\n",
    "Kod poniżej ma za zadanie zilutrować działanie kodu tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "118 words into vocab.tsv\n",
      "This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels. --> [ 42  12  41 118  33 119  51  47 118 119  96  39 112  54   1  87 111  56\n",
      " 119  81  19 119  23  87 117  99  47 119]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import lookup\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "print tf.__version__\n",
    "MAX_DOCUMENT_LENGTH = 100000  \n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# vocabulary\n",
    "lines = ['This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels.', \n",
    "         'Im always looking for a good place to sing karaoke. This is one of them. Drinks are cheap. Food is delicious. And its laid back and fun. I shall return!', \n",
    "         'Crazy Man just Crazy there are like 3 different places called Saigon Deli at this intersection but this one is on Jackson just west of twelfthand with $2 pork Banh Mi you cant go wrong, I am going to Indiana for work so I went in and got 6 Pork', \n",
    "         'Ive eaten here a few times in the past and thought it was decent. I went last night, however, and our meal was really subpar so the place seems to have gone down hill.We had chow fun noodles and the hollow vegetables with chili sauce']\n",
    "\n",
    "# create vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(lines)\n",
    "with gfile.Open('vocab.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.iteritems():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "N_WORDS = len(vocab_processor.vocabulary_)\n",
    "print '{} words into vocab.tsv'.format(N_WORDS)\n",
    "\n",
    "# can use the vocabulary to convert words to numbers\n",
    "table = lookup.index_table_from_file(\n",
    "  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
    "numbers = table.lookup(tf.constant(lines[0].split()))\n",
    "with tf.Session() as sess:\n",
    "  tf.tables_initializer().run()\n",
    "  print \"{} --> {}\".format(lines[0], numbers.eval())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZYXW\r\n",
      "shop\r\n",
      "just\r\n",
      "cheap\r\n",
      "go\r\n",
      "Indiana\r\n",
      "its\r\n",
      "We\r\n",
      "seems\r\n",
      "laid\r\n"
     ]
    }
   ],
   "source": [
    "!head vocab.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews= [ 'This might be the best \"taco\" truck on the planet. Hidden between a smoke shop and The Home Depot, this semi permanent, stylish and clean cafe on wheels.'\n",
      " 'Im always looking for a good place to sing karaoke. This is one of them. Drinks are cheap. Food is delicious. And its laid back and fun. I shall return!'\n",
      " 'Crazy Man just Crazy there are like 3 different places called Saigon Deli at this intersection but this one is on Jackson just west of twelfthand with $2 pork Banh Mi you cant go wrong, I am going to Indiana for work so I went in and got 6 Pork'\n",
      " 'Ive eaten here a few times in the past and thought it was decent. I went last night, however, and our meal was really subpar so the place seems to have gone down hill.We had chow fun noodles and the hollow vegetables with chili sauce'] (4,)\n",
      "words= SparseTensorValue(indices=array([[ 0,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0,  2],\n",
      "       [ 0,  3],\n",
      "       [ 0,  4],\n",
      "       [ 0,  5],\n",
      "       [ 0,  6],\n",
      "       [ 0,  7],\n",
      "       [ 0,  8],\n",
      "       [ 0,  9],\n",
      "       [ 0, 10],\n",
      "       [ 0, 11],\n",
      "       [ 0, 12],\n",
      "       [ 0, 13],\n",
      "       [ 0, 14],\n",
      "       [ 0, 15],\n",
      "       [ 0, 16],\n",
      "       [ 0, 17],\n",
      "       [ 0, 18],\n",
      "       [ 0, 19],\n",
      "       [ 0, 20],\n",
      "       [ 0, 21],\n",
      "       [ 0, 22],\n",
      "       [ 0, 23],\n",
      "       [ 0, 24],\n",
      "       [ 0, 25],\n",
      "       [ 0, 26],\n",
      "       [ 0, 27],\n",
      "       [ 1,  0],\n",
      "       [ 1,  1],\n",
      "       [ 1,  2],\n",
      "       [ 1,  3],\n",
      "       [ 1,  4],\n",
      "       [ 1,  5],\n",
      "       [ 1,  6],\n",
      "       [ 1,  7],\n",
      "       [ 1,  8],\n",
      "       [ 1,  9],\n",
      "       [ 1, 10],\n",
      "       [ 1, 11],\n",
      "       [ 1, 12],\n",
      "       [ 1, 13],\n",
      "       [ 1, 14],\n",
      "       [ 1, 15],\n",
      "       [ 1, 16],\n",
      "       [ 1, 17],\n",
      "       [ 1, 18],\n",
      "       [ 1, 19],\n",
      "       [ 1, 20],\n",
      "       [ 1, 21],\n",
      "       [ 1, 22],\n",
      "       [ 1, 23],\n",
      "       [ 1, 24],\n",
      "       [ 1, 25],\n",
      "       [ 1, 26],\n",
      "       [ 1, 27],\n",
      "       [ 1, 28],\n",
      "       [ 1, 29],\n",
      "       [ 2,  0],\n",
      "       [ 2,  1],\n",
      "       [ 2,  2],\n",
      "       [ 2,  3],\n",
      "       [ 2,  4],\n",
      "       [ 2,  5],\n",
      "       [ 2,  6],\n",
      "       [ 2,  7],\n",
      "       [ 2,  8],\n",
      "       [ 2,  9],\n",
      "       [ 2, 10],\n",
      "       [ 2, 11],\n",
      "       [ 2, 12],\n",
      "       [ 2, 13],\n",
      "       [ 2, 14],\n",
      "       [ 2, 15],\n",
      "       [ 2, 16],\n",
      "       [ 2, 17],\n",
      "       [ 2, 18],\n",
      "       [ 2, 19],\n",
      "       [ 2, 20],\n",
      "       [ 2, 21],\n",
      "       [ 2, 22],\n",
      "       [ 2, 23],\n",
      "       [ 2, 24],\n",
      "       [ 2, 25],\n",
      "       [ 2, 26],\n",
      "       [ 2, 27],\n",
      "       [ 2, 28],\n",
      "       [ 2, 29],\n",
      "       [ 2, 30],\n",
      "       [ 2, 31],\n",
      "       [ 2, 32],\n",
      "       [ 2, 33],\n",
      "       [ 2, 34],\n",
      "       [ 2, 35],\n",
      "       [ 2, 36],\n",
      "       [ 2, 37],\n",
      "       [ 2, 38],\n",
      "       [ 2, 39],\n",
      "       [ 2, 40],\n",
      "       [ 2, 41],\n",
      "       [ 2, 42],\n",
      "       [ 2, 43],\n",
      "       [ 2, 44],\n",
      "       [ 2, 45],\n",
      "       [ 2, 46],\n",
      "       [ 2, 47],\n",
      "       [ 2, 48],\n",
      "       [ 2, 49],\n",
      "       [ 3,  0],\n",
      "       [ 3,  1],\n",
      "       [ 3,  2],\n",
      "       [ 3,  3],\n",
      "       [ 3,  4],\n",
      "       [ 3,  5],\n",
      "       [ 3,  6],\n",
      "       [ 3,  7],\n",
      "       [ 3,  8],\n",
      "       [ 3,  9],\n",
      "       [ 3, 10],\n",
      "       [ 3, 11],\n",
      "       [ 3, 12],\n",
      "       [ 3, 13],\n",
      "       [ 3, 14],\n",
      "       [ 3, 15],\n",
      "       [ 3, 16],\n",
      "       [ 3, 17],\n",
      "       [ 3, 18],\n",
      "       [ 3, 19],\n",
      "       [ 3, 20],\n",
      "       [ 3, 21],\n",
      "       [ 3, 22],\n",
      "       [ 3, 23],\n",
      "       [ 3, 24],\n",
      "       [ 3, 25],\n",
      "       [ 3, 26],\n",
      "       [ 3, 27],\n",
      "       [ 3, 28],\n",
      "       [ 3, 29],\n",
      "       [ 3, 30],\n",
      "       [ 3, 31],\n",
      "       [ 3, 32],\n",
      "       [ 3, 33],\n",
      "       [ 3, 34],\n",
      "       [ 3, 35],\n",
      "       [ 3, 36],\n",
      "       [ 3, 37],\n",
      "       [ 3, 38],\n",
      "       [ 3, 39],\n",
      "       [ 3, 40],\n",
      "       [ 3, 41],\n",
      "       [ 3, 42],\n",
      "       [ 3, 43],\n",
      "       [ 3, 44]]), values=array(['This', 'might', 'be', 'the', 'best', '\"taco\"', 'truck', 'on',\n",
      "       'the', 'planet.', 'Hidden', 'between', 'a', 'smoke', 'shop', 'and',\n",
      "       'The', 'Home', 'Depot,', 'this', 'semi', 'permanent,', 'stylish',\n",
      "       'and', 'clean', 'cafe', 'on', 'wheels.', 'Im', 'always', 'looking',\n",
      "       'for', 'a', 'good', 'place', 'to', 'sing', 'karaoke.', 'This', 'is',\n",
      "       'one', 'of', 'them.', 'Drinks', 'are', 'cheap.', 'Food', 'is',\n",
      "       'delicious.', 'And', 'its', 'laid', 'back', 'and', 'fun.', 'I',\n",
      "       'shall', 'return!', 'Crazy', 'Man', 'just', 'Crazy', 'there', 'are',\n",
      "       'like', '3', 'different', 'places', 'called', 'Saigon', 'Deli',\n",
      "       'at', 'this', 'intersection', 'but', 'this', 'one', 'is', 'on',\n",
      "       'Jackson', 'just', 'west', 'of', 'twelfthand', 'with', '$2', 'pork',\n",
      "       'Banh', 'Mi', 'you', 'cant', 'go', 'wrong,', 'I', 'am', 'going',\n",
      "       'to', 'Indiana', 'for', 'work', 'so', 'I', 'went', 'in', 'and',\n",
      "       'got', '6', 'Pork', 'Ive', 'eaten', 'here', 'a', 'few', 'times',\n",
      "       'in', 'the', 'past', 'and', 'thought', 'it', 'was', 'decent.', 'I',\n",
      "       'went', 'last', 'night,', 'however,', 'and', 'our', 'meal', 'was',\n",
      "       'really', 'subpar', 'so', 'the', 'place', 'seems', 'to', 'have',\n",
      "       'gone', 'down', 'hill.We', 'had', 'chow', 'fun', 'noodles', 'and',\n",
      "       'the', 'hollow', 'vegetables', 'with', 'chili', 'sauce'], dtype=object), dense_shape=array([ 4, 50]))\n",
      "dense= [['This' 'might' 'be' 'the' 'best' '\"taco\"' 'truck' 'on' 'the' 'planet.'\n",
      "  'Hidden' 'between' 'a' 'smoke' 'shop' 'and' 'The' 'Home' 'Depot,' 'this'\n",
      "  'semi' 'permanent,' 'stylish' 'and' 'clean' 'cafe' 'on' 'wheels.' 'ZYXW'\n",
      "  'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW'\n",
      "  'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW'\n",
      "  'ZYXW']\n",
      " ['Im' 'always' 'looking' 'for' 'a' 'good' 'place' 'to' 'sing' 'karaoke.'\n",
      "  'This' 'is' 'one' 'of' 'them.' 'Drinks' 'are' 'cheap.' 'Food' 'is'\n",
      "  'delicious.' 'And' 'its' 'laid' 'back' 'and' 'fun.' 'I' 'shall' 'return!'\n",
      "  'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW'\n",
      "  'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW']\n",
      " ['Crazy' 'Man' 'just' 'Crazy' 'there' 'are' 'like' '3' 'different'\n",
      "  'places' 'called' 'Saigon' 'Deli' 'at' 'this' 'intersection' 'but' 'this'\n",
      "  'one' 'is' 'on' 'Jackson' 'just' 'west' 'of' 'twelfthand' 'with' '$2'\n",
      "  'pork' 'Banh' 'Mi' 'you' 'cant' 'go' 'wrong,' 'I' 'am' 'going' 'to'\n",
      "  'Indiana' 'for' 'work' 'so' 'I' 'went' 'in' 'and' 'got' '6' 'Pork']\n",
      " ['Ive' 'eaten' 'here' 'a' 'few' 'times' 'in' 'the' 'past' 'and' 'thought'\n",
      "  'it' 'was' 'decent.' 'I' 'went' 'last' 'night,' 'however,' 'and' 'our'\n",
      "  'meal' 'was' 'really' 'subpar' 'so' 'the' 'place' 'seems' 'to' 'have'\n",
      "  'gone' 'down' 'hill.We' 'had' 'chow' 'fun' 'noodles' 'and' 'the' 'hollow'\n",
      "  'vegetables' 'with' 'chili' 'sauce' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW' 'ZYXW']] (?, ?)\n",
      "numbers= [[ 42  12  41 118  33 119  51  47 118 119  96  39 112  54   1  87 111  56\n",
      "  119  81  19 119  23  87 117  99  47 119   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [116  20  36  35 112  14  53  10  46 119  42  89  62  50 119  58  30 119\n",
      "  100  89 119  77   6   9  28  87 119 106 105 119   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 79 104   2  79  68  30  18  37  98  80  86  88 115  93  81  17  73  81\n",
      "   62  89  47  27   2  66  50  49  75 119  26 103  43 102  92   4 119 106\n",
      "   90  70  10   5  35  83 101 106  24  95  87  40  71 113]\n",
      " [ 78 109  45 112  69  52  95 118  29  87 114  91  72 119 106  24  48 119\n",
      "  119  87  31  25  72  34  59 101 118  53   8  10  94  60  63 119  61 107\n",
      "   85  32  87 118  21  67  75  74  82   0   0   0   0   0]] (?, ?)\n",
      "padding= [[     0      0]\n",
      " [     0 100000]] (2, 2)\n",
      "padded= [[ 42  12  41 ...,   0   0   0]\n",
      " [116  20  36 ...,   0   0   0]\n",
      " [ 79 104   2 ...,   0   0   0]\n",
      " [ 78 109  45 ...,   0   0   0]] (?, ?)\n",
      "sliced= [[ 42  12  41 ...,   0   0   0]\n",
      " [116  20  36 ...,   0   0   0]\n",
      " [ 79 104   2 ...,   0   0   0]\n",
      " [ 78 109  45 ...,   0   0   0]] (?, 100000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-17 13:39:27.787211. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# string operations\n",
    "reviews = tf.constant(lines)\n",
    "words = tf.string_split(reviews)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "\n",
    "# now pad out with zeros and then slice to constant length\n",
    "padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "padded = tf.pad(numbers, padding)\n",
    "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.tables_initializer().run()\n",
    "  print \"reviews=\", reviews.eval(), reviews.shape\n",
    "  print \"words=\", words.eval()\n",
    "  print \"dense=\", densewords.eval(), densewords.shape\n",
    "  print \"numbers=\", numbers.eval(), numbers.shape\n",
    "  print \"padding=\", padding.eval(), padding.shape\n",
    "  print \"padded=\", padded.eval(), padded.shape\n",
    "  print \"sliced=\", sliced.eval(), sliced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie działania modelu na małym zbiorze danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=dswbiznesie\n",
      "5687 words into gs://dswbiznesie/data/vocab_words\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 15568), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 15568), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "{'review_count': <tf.Tensor 'DecodeCSV:4' shape=(?, 1) dtype=float32>, 'avg_rating': <tf.Tensor 'DecodeCSV:5' shape=(?, 1) dtype=float32>, 'reviews': <tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=string>, 'categories': <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, 'zip_code': <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>} Tensor(\"hash_table_Lookup:0\", shape=(?, 1), dtype=int64)\n",
      "Tensor(\"string_to_index_1_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 15568), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "Tensor(\"DecodeCSV:5\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"DecodeCSV:4\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"string_to_index_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64))\n",
      "words_embed=Tensor(\"EmbedSequence/embedding_lookup:0\", shape=(?, 77838, 10), dtype=float32)\n",
      "words_conv=Tensor(\"Squeeze_1:0\", shape=(?, 15568), dtype=float32)\n",
      "embed_categories shape\n",
      "(?, 100, 10)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_2:0\", shape=(?, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://dswbiznesie/data/train.csv...\n",
      "/ [1 files][  3.2 MiB/  3.2 MiB]                                                \n",
      "Operation completed over 1 objects/3.2 MiB.                                      \n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb92f412e90>, '_model_dir': 'outputdir/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-11-11 18:14:43.935760: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-11 18:14:43.935833: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-11-11 18:14:43.935854: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into outputdir/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.688263, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-11-18:14:45\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-11-18:15:06\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.498044, f1score = 0.664922, global_step = 1, loss = 0.720355, precision = 0.498044, recall = 1.0\n",
      "INFO:tensorflow:Validation (step 1): loss = 0.720355, global_step = 1, recall = 1.0, precision = 0.498044, f1score = 0.664922, accuracy = 0.498044\n",
      "INFO:tensorflow:global_step/sec: 0.904262\n",
      "INFO:tensorflow:loss = 0.668164, step = 101 (110.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.90497\n",
      "INFO:tensorflow:loss = 0.694582, step = 201 (52.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92633\n",
      "INFO:tensorflow:loss = 0.702295, step = 301 (51.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.91537\n",
      "INFO:tensorflow:loss = 0.680249, step = 401 (52.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.90204\n",
      "INFO:tensorflow:loss = 0.661024, step = 501 (52.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.92446\n",
      "INFO:tensorflow:loss = 0.45132, step = 601 (51.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.88484\n",
      "INFO:tensorflow:loss = 0.159901, step = 701 (53.055 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into outputdir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0979083.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-11-18:22:43\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-800\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-11-18:23:04\n",
      "INFO:tensorflow:Saving dict for global step 800: accuracy = 0.591917, f1score = 0.500793, global_step = 800, loss = 0.994444, precision = 0.640816, recall = 0.410995\n",
      "INFO:tensorflow:Restoring parameters from outputdir/model.ckpt-800\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: outputdir/export/Servo/1510424584/assets\n",
      "INFO:tensorflow:SavedModel written to: outputdir/export/Servo/1510424584/saved_model.pb\n",
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-11 18:14:38.917629. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf outputdir\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/trainer\n",
    "python -m trainer.task \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trenowanie modelu w Cloud ML </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać kod pythonowy działa przy bezpośrednim wywołaniu. Można przetestować go lokalnie w Cloud ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${REPO}/notebooks/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${REPO}/notebooks/trained_model \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --train_steps=800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Wizualizacja procesu uczenia w Tensorboard </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 5268. Click <a href=\"/_proxy/43452/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-11 18:00:48.963627. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('{}/notebooks/outputdir'.format(REPO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak przygotowany kod można uruchomić w klastrze Cloud ML. Status joba uczącego znajduje się w konsoli GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dswbiznesie/trained_model europe-west1 hygiene_171111_161537\n",
      "jobId: hygiene_171111_161537\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://dswbiznesie/trained_model/__init__.py#1510416808672460...\n",
      "Removing gs://dswbiznesie/trained_model/model.py#1510416808944571...\n",
      "Removing gs://dswbiznesie/trained_model/task.py#1510416809177912...\n",
      "/ [3/3 objects] 100% Done                                                       \n",
      "Operation completed over 3 objects.                                              \n",
      "Copying file://trainer/__init__.py [Content-Type=text/x-python]...\n",
      "Copying file://trainer/model.py [Content-Type=text/x-python]...\n",
      "Copying file://trainer/task.py [Content-Type=text/x-python]...\n",
      "-\n",
      "Operation completed over 3 objects/12.8 KiB.                                     \n",
      "Job [hygiene_171111_161537] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hygiene_171111_161537\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hygiene_171111_161537\n",
      "/usr/local/lib/python2.7/dist-packages/simplejson/encoder.py:291: DeprecationWarning: Interpreting naive datetime as local 2017-11-11 16:15:37.157629. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/trained_model\n",
    "JOBNAME=hygiene_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gsutil cp trainer/*.py $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC --runtime-version=1.2 \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --train_steps=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Opublikowanie wytrenowanego modelu </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/trained_model/export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"hygiene\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/trained_model/export/Servo/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Serwowanie predykcji </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opublikowany model, zostanie użyty do serwowania predykcji. Aby otrzymać predykcje, wystarczy wykonać JSONowy request do api predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud ml-engine predict --model=hygiene --version=v1 --json-instances=./not-passed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {\"instances\":\n",
    "  [\n",
    "      {\n",
    "      \"reviews\":\" Great food, friendly staff, go go go go go! :) Ah, we always go here everytime we go to Pike Place. I always get either the Halibut sandwich or the Halibut platter.The sandwich comes in a french bread from Le Panier, delicious. The fish is fresh and why wouldn't it be? The Market Grill is inside pike place across from all the fish vendors! The cook takes it out and grills it in front of you with an option to add the blackening seasoning. I always get that. The platter comes with garlic bread, organic brown rice and organic salad. The rice is slightly flavored from the delicious dressing they have from the salad and it's soooo good. The fish is also very good.I've also had their clam chowder and I like it also.Ah, maybe I'll stop by this weekend. It's also a fun atmosphere because you can sit on the stools and watch people as you enjoy your fish. I've recommended this place to everyone I know! Especially if they're looking for something quick, cheaper and delicious in Pike Place. I've tried almost all of their sandwiches and I can honestly say they have been great. I've yet to try the chicken sandwich, but when your other options are salmon, halibut and prawns it's kind of hard to do so. It's totally worth grabbing the sandwich and walking around the market for a spot on a bench since the stools in their nook are almost always packed. I think there's only 8 of them. The salmon is probably my favorite but you really can't go wrong here. The chowder is also worth the wait. Make sure to treat yourself to the sides of chowder and coleslaw. You won't be disappointed. The staff here is usually very funny and talkative. They'll make you feel right at home, carrying on conversation while flipping your food on the grill just a few feet away from the counter. I bring my friends here whenever someone comes to visit and they all love it too. Just got back from a short mid-winter stay in Seattle. &#160;The weather was beautiful and we walked down to the Market for some lunch armed with several Yelp recommendations.We ended up at the Market Grill. &#160;Of course the place was crowded. &#160;We could have waited a few and grabbed one of the stools but opted to order take-out and eat in the park.In spite of some of the reviews indicating the sandwiches were a bit small, they looked big enough for us to share. &#160;We went with the salmon, blackened, and loaded with the onions, and rosmary mayo, lettuce, and tomatoes, two sides of slaw, and a medium chowder to share and headed to the park to eat our lunch. &#160;Total price $20.00I'm telling you, this is a damn good sandwich. &#160;Every component worked together from the crunchy baguette, the perfect tomatoes, the crisp lettuce, the MAYO (yum), and of course the huge piece of fresh perfectly cooked fish. &#160;The Slaw was crisp, chunky, and very flavorful, with the right amount of heat. &#160;The chowder was also very good. &#160;Definitely home made. &#160;Possibly a bit light on the clams, but then again, we finished the whole thing and were basically licking the bowl.We will come back - for sure.\"\n",
    "       },\n",
    "    {\n",
    "    \"reviews\":\" Pam's is really great. &#160;It's simple and delicious food that was perfect for a cold Seattle night that needed a little spicing up. &#160;I've only had the beef roti &#160;but I was so full that I couldn't fit any appetizer or dessert which I usually have room for. The beef roti was filled with curried chick peas, potatoes, large chunks of beef, and a yummy sauce. &#160;I can't wait to go back and try the other menu items. Also, A+ for service that was efficient, kind, and not overwhelming! Despite it's unassuming storefront facade, this is a very impressive little restaurant. &#160;Two of us tried two different dishes - lamb roti and a brown rice with roast peas - and both were perfect. For dessert we had an unusual and lovely little cake of coconut and casava root. The menu is simple, but every dish is excellent. &#160;It can't be easy to combine so many different influences (Carribean, African, East Indian) and still have the food taste so good and so subtle. Plus, on the rainy day that I was there, the mix of tropical colors, warm smells and Carribean music were a welcome change of atmosphere. I'm looking forward to going back. Really good food. When I ate meat, I tried both the chicken and beef. Both were really good. Now that I'm a vegetarian, I just eat the potato and chickpea mix and that is really good too. I've tried both styles of roti (wrapped up like a burrito and on the side like a big tortilla) and both are really good. The habanero hot sauce is amazing. I love it! Service is good too. I just wish it was closer to my apartment so I could eat there more often. The menu is limited, but what they do have is really good. They have perfected a small menu, which I think is much better than having a big mediocre menu. I find myself in Pam's Kitchen at least once per month, and am continuously amazed the place is not packed with people. &#160;The food is uniquely delicious and always exceptionally prepared - I usually order a Chicken Roti (Roti = chick peas potatoes and onions baked together with choice of meat and powerful blend of Caribbean spice, then wrapped in savory pastry) and one of their strangely addicting milk based punches, which come in peanut, carrot and (my favorite) pumpkin. &#160;At $15 for those two items (including tax and tip) it is a pretty good deal, though i realize you could get two mediocre teryaki meals from Tokyo Garden for the roughly the same price. &#160;They have a good selection of beer to fit the menu, service is great, and the island atmosphere is undeniably cool. &#160; Hungry? &#160;Thats good - I've never left Pam's feeling less than comfortably stuffed. &#160;The place is perfect for groups of three or four, so bring some friends and impress them with a creative and international dining experience on a student's budget. This place was fantastic! &#160;The owners were very nice and attentive--he even pointed out the Yelp! sticker on his window--what a business man!My friend and I decided this place looked interesting and we'd never had this kind of food before. &#160;Basically, we came looking for adventure and left satisfied. &#160;We split the Lamb Roti and were incredibly full. &#160;My friend thought it should be more saucy, but I was really happy w/it. &#160; &#160;Plus, the music is fantastic. &#160;At first I commented that it was way too loud, but then I kept dancing in my chair. &#160;It reminded me of some cheesy Bollywood-esque score. I'll admit that I'm naive about roti. &#160;So, I must make two disclaimers.First, I have no idea whether I'm being ripped off. &#160;I've read that, by island standards, this is way-expensive. &#160;Then again, this is Seattle, and living costs tend to make *everything* more expensive than on the streets of Chaguanas. &#160;It's not a steal (by even Seattle standards), but it also doesn't seem to be excessive.Second, I don't know how this holds up to roti in the Caribbean. &#160;It's hard to imagine that flaky goodness gets any flakier or goodness-er than this, but I hope it doesn't, because I fear for my carbohydrate intake once something better opens up.As you can probably already tell, I enjoyed Pam's Kitchen. &#160;I had the goat roti, with the version where some assembly is required (as opposed to the pre-filled roti, this one comes with all components separated). &#160;The goat was tender and reasonably juicy, and made for a perfect filling. &#160;I also got a spicy pumpkin side ($3), which was exactly as advertised, and a good complement to the main dish.Service for me was, contrary to some reviews, exemplary. &#160;They go by a sit-where-you-like system, but I was addressed promptly. &#160;I was also entertained by the cook briefly, who explained the positive medicinal effects of roti. &#160;I'm not sure if roti is responsible for curing all of the listed diseases, but if it is, I demand U.N. funding immediately, straight to my house.As mentioned, prices are moderate. &#160;Roti will run you $8 to $10, which seems high for the University District, but is about what this sort of food would run you elsewhere in town. &#160;Certainly, not enough to keep me away, but probably enough to deny me the prophylactic powers of Pam's delicacies.\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" OMG the spicy Teriyaki almost killed me today. &#160;I order it all the time and love it but this time it was like eating a fireball! &#160;I called to complain and they said \\\"next time we make not spicy\\\" &#160;What the heck? &#160;Also, someone please tell the cashier dude to clean his finger nails... Grosssssssss Hmmm should I go to Subway like always or try something new? Eh I might as well try something new. However, my first glance inside Teriyaki Plus isn't too encouraging as I don't see a soul inside. Eh, there are always rushes and slow periods. I go inside since there is no menu outside to look at. Immediately the cashier comes out to stare at me as if guilting me into buying something, instead of walking out after seeing the interior (dilapidated with dirty floors, and walls, along with tables and chairs that clearly need to be replaced). I order the white chicken and beef combo so that I can try each meat (in case one isn't very good). After waiting about 15 minutes (and getting annoyed because I got takeout since I'm in a hurry and yet have been waiting as long as I would have at a normal restaurant). The cashier comes out with my food and asks \\\"Fork?\\\" while motioning eating with a fork. I left with chopsticks (yes I'm a white girl who can use chopsticks) and my food in a bag. Upon opening the bag I noticed they used compostable containers which is nice, however the dressing from the salad had leaked into my bag. I opened the container and realized they have given me a TON of salad drenched in an overly sweet creamy sauce. Maybe they thought I needed more vegetables in my diet? While they also gave me a lot of rice, the meat was lacking compared to most teriyaki places. Which is fine as long as the meat is quality. However, the beef was barely tolerable and the chicken was even drier than the beef. Did they perhaps forget to add sauce? I never drench my food in anything, but this called for drastic measures. After drenching all the meat in soy sauce it was slight less dehydrated, but still as tough and chewy as before. My conclusion: how exactly is this place still open? Is it due to stupid people like me who don't check Yelp when they're starving and in a hurry? Well, back to good old Subway for me in times of desperate takeout (or Qdoba across the street).\"\n",
    "      },\n",
    "      {\n",
    "        \"reviews\":\" Nothing is fresh about it! The quality of food is lower then the worst fast food place you've ever been to (and the smell is awful). Its dirty and If you are used to eating somewhat healthy you will feel sick. The reason this place has any customers at all is because for three 20+ stores office buildings there are only 4 places to eat. The way the food is made grosses me out; when I watch things being slapped on top of each other I wish I had no visibility into their kitchen. On top of that because people have limited options they charge a lot for what they offer.\"\n",
    "      },\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'hygiene', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
